\begin{abstract}
Finite-horizon networked control tasks require explicit trade-offs between estimation quality and communication usage.
We study transmission scheduling for discrete-time linear--Gaussian systems operating over packet-dropping channels with acknowledgements.
Acknowledgements reveal whether a transmitted measurement is received, which makes the Kalman estimation error covariance a fully observed information state for communication decisions.
Using this structure, we cast finite-horizon scheduling as a Markov decision process (MDP) whose state is the covariance and whose action is a binary transmit decision.
We derive the finite-horizon Bellman recursion and an optimal time-varying policy $\gamma_k^\star(P)$ characterized by a value-of-information test.
Because the covariance state is continuous, we describe an offline dynamic-programming procedure based on gridding low-dimensional features of the covariance, yielding an oracle benchmark for finite-horizon performance--communication limits.
Numerical studies illustrate intrinsic finite-horizon effects and quantify gaps between covariance-aware scheduling and common scalar-surrogate heuristics.
\end{abstract}

\begin{IEEEkeywords}
networked control systems, remote estimation, transmission scheduling, Markov decision processes, packet drops, finite-horizon optimization
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
Wireless sensing and actuation links are fundamental components of modern cyber--physical systems.
In many applications, sensor measurements must be delivered to a controller over shared and unreliable wireless channels.
Periodic transmission can be wasteful under bandwidth or energy constraints, while unreliable links with packet drops make open-loop scheduling brittle.
This motivates \emph{transmission scheduling}: deciding \emph{when} to transmit measurements so as to balance control/estimation performance against communication usage.

Most existing scheduling formulations emphasize asymptotic stability or infinite-horizon average performance.
However, many practical scenarios are mission-driven and inherently finite-horizon: a robot must traverse a corridor in $30$\,s, an inspection drone has a short battery window, or a sensor network must complete a sweep under a limited packet budget.
In such settings, the \emph{remaining time} matters, and optimal scheduling decisions are generally time-varying.

This paper focuses on finite-horizon scheduling for linear systems over packet-dropping channels with acknowledgements (ACKs).
Under linear--Gaussian assumptions, ACKs allow the controller to track the Kalman estimation error covariance exactly.
We exploit this property to formulate scheduling as a finite-horizon MDP on the covariance information state and to compute an MDP-optimal (oracle) policy.
The objective is not to propose a low-complexity deployable scheduler, but to provide a principled finite-horizon benchmark and a clear computational recipe that can be used to design and evaluate practical heuristics.

\paragraph*{Contributions}
\begin{itemize}
\item We formulate finite-horizon transmission scheduling with ACKs as a fully observed covariance-state MDP.
\item We derive a finite-horizon dynamic program and an optimal policy $\gamma_k^\star(P)$ with an explicit value-of-information comparison.
\item We present an offline approximation procedure (feature gridding + backward induction) that produces an oracle benchmark and supports reproducible finite-horizon trade-off studies.
\end{itemize}

\section{Related Work}\label{sec:related}
Transmission scheduling and remote estimation over unreliable channels have been studied extensively in networked control and estimation.
Classic results analyze Kalman filtering under packet drops and identify conditions for bounded estimation error.
MDP-based sensor scheduling is also well developed under infinite-horizon discounted or average-cost criteria.
Event-triggered and self-triggered strategies provide low-complexity alternatives by transmitting only when a trigger condition is met, often yielding stability and communication savings.
In contrast, finite-horizon formulations explicitly encode the remaining time and can exhibit qualitatively different, time-varying optimal decisions.
This paper emphasizes finite-horizon benchmarking through a covariance-state MDP and an oracle dynamic program.

\section{System Model and Information State}\label{sec:model}

\subsection{Plant, Measurements, and Certainty-Equivalent Control}
We consider the discrete-time LTI system
\begin{equation}
x_{k+1} = A x_k + B u_k + w_k,
\label{eq:lti}
\end{equation}
where $x_k\in\mathbb{R}^n$, $u_k\in\mathbb{R}^m$, and $\{w_k\}$ is zero-mean i.i.d.\ process noise with covariance $Q\succeq 0$.
The sensor measures
\begin{equation}
y_k = C x_k + v_k,
\label{eq:meas}
\end{equation}
where $\{v_k\}$ is zero-mean i.i.d.\ measurement noise with covariance $R\succ 0$.
We assume $(A,B)$ is stabilizable and $(A,C)$ is detectable.

The controller applies certainty-equivalent state feedback
\begin{equation}
u_k = -K \hat{x}_k,
\label{eq:ce_control}
\end{equation}
where $\hat{x}_k$ is the controller-side estimate and $K$ is chosen such that $A-BK$ is stable.

\subsection{Packet Drops and Acknowledgements}
At each time $k$, the scheduler chooses a binary transmission decision
\begin{equation}
\gamma_k \in \{0,1\},
\label{eq:gamma_decision}
\end{equation}
where $\gamma_k=1$ requests transmission of the next measurement and $\gamma_k=0$ remains silent.
Packet delivery is modeled by an i.i.d.\ Bernoulli process
\begin{equation}
\eta_k \in \{0,1\},\qquad \Pr(\eta_k=1)=p,
\label{eq:drop}
\end{equation}
independent across time and independent of $(w_k,v_k)$.
The controller observes an ACK, i.e., it knows the \emph{effective reception indicator}
\begin{equation}
\delta_k := \gamma_k \eta_k.
\label{eq:delta}
\end{equation}
Thus $\delta_k=1$ if and only if a transmission is attempted and successfully received.
\paragraph*{Acknowledgements and observability}
We assume ACKs are available to the decision maker before the next scheduling decision.
Hence $\delta_k$ is observed, and the controller can update the covariance exactly.
Without ACKs, the scheduler would not know whether the measurement update occurred, and the scheduling problem becomes partially observed.

\subsection{Intermittent Kalman Filtering and Covariance Dynamics}\label{sec:kf}
The controller maintains a Kalman filter driven by intermittently received measurements.
Let $P_k$ denote the controller-side estimation error covariance at the decision time $k$,
\begin{equation}
P_k := \mathbb{E}\!\left[(x_k-\hat{x}_k)(x_k-\hat{x}_k)^\top\right].
\label{eq:P_def}
\end{equation}
Given $P_k$, the one-step predicted covariance is
\begin{equation}
\mathcal{T}(P_k) := A P_k A^\top + Q,
\label{eq:T_op}
\end{equation}
and the corresponding measurement-update operator is
\begin{equation}
\mathcal{M}(P) := P - P C^\top (C P C^\top + R)^{-1} C P.
\label{eq:M_op}
\end{equation}
\paragraph*{Timing convention}
Starting from $P_k$, the controller forms the predicted covariance $\mathcal{T}(P_k)$.
If $\delta_k=1$, it applies the measurement update to obtain $\mathcal{M}(\mathcal{T}(P_k))$; otherwise no update occurs.
We denote the resulting covariance by $P_{k+1}$.
Using the ACK $\delta_k$, the covariance evolves as the controlled Markov recursion
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{T}(P_k), & \delta_k=0,\\[1mm]
\mathcal{M}\!\big(\mathcal{T}(P_k)\big), & \delta_k=1.
\end{cases}
\label{eq:P_update}
\end{equation}

\paragraph*{Information state}
Because ACKs reveal $\delta_k$, the controller can track $P_k$ exactly via~\eqref{eq:P_update}.
Moreover, under the linear--Gaussian model, $(P_k)$ is a sufficient information state for scheduling: for any causal policy, the conditional distribution of future estimation errors depends on the past only through $P_k$.

\section{Finite-Horizon Covariance-State MDP}\label{sec:mdp}
We now formulate transmission scheduling as a finite-horizon MDP with continuous state $P_k$.

\subsection{MDP Components}
\paragraph*{State}
$s_k:=P_k\in\mathbb{S}_+^n$, where $\mathbb{S}_+^n$ denotes the cone of symmetric positive semidefinite matrices.

\paragraph*{Action} the binary scheduling decision $\gamma_k\in\{0,1\}$.

\paragraph*{Transition} given $(P_k,\gamma_k)$, the next covariance follows~\eqref{eq:P_update} with $\delta_k=\gamma_k\eta_k$ and $\eta_k\sim\mathrm{Bernoulli}(p)$.
Equivalently, for $\gamma_k=0$ the transition is deterministic $P_{k+1}=\mathcal{T}(P_k)$, while for $\gamma_k=1$ the next state is $\mathcal{M}(\mathcal{T}(P_k))$ with probability $p$ and $\mathcal{T}(P_k)$ with probability $1-p$.

\paragraph*{Stage cost}
We consider the finite-horizon additive cost
\begin{equation}
c(P_k,\gamma_k) := \mathrm{tr}(W P_k) + \lambda\,\gamma_k,
\label{eq:stage_cost}
\end{equation}
where $W\succeq 0$ weights estimation uncertainty and $\lambda>0$ weights communication usage.
The common choice $W=I$ yields $c(P,\gamma)=\mathrm{tr}(P)+\lambda\gamma$.
\paragraph*{Remark (estimation-focused objective)}
We use an estimation-only term $\mathrm{tr}(W P_k)$ to isolate the impact of communication decisions on estimation quality and to obtain a covariance-state MDP.
While estimation quality directly affects closed-loop behavior under certainty-equivalent control, incorporating a full finite-horizon LQG regulation cost would generally require tracking additional second-order moments (beyond $P_k$) and would substantially change the problem formulation.
Accordingly, we treat the controller gain $K$ as fixed and focus on estimation-driven scheduling; we report a closed-loop proxy metric in Section~\ref{sec:results} to connect the benchmark to regulation performance.

\subsection{Finite-Horizon Problem Statement}\label{sec:problem}
Fix a horizon $T$.
A causal scheduling policy is a sequence $\pi=\{\pi_0,\ldots,\pi_{T-1}\}$ with
\begin{equation}
\gamma_k = \pi_k(P_k),\qquad k=0,\ldots,T-1.
\label{eq:policy_def}
\end{equation}
We study the finite-horizon optimization problem
\begin{equation}
\min_{\pi}\ 
\mathbb{E}^{\pi}\!\left[
\sum_{k=0}^{T-1} \big(\mathrm{tr}(W P_k) + \lambda\,\gamma_k\big)
\right],
\label{eq:mdp_problem}
\end{equation}
subject to the covariance dynamics~\eqref{eq:P_update} and a given initial covariance $P_0$.

\paragraph*{Remark (soft vs.\ hard budgets)}
The weighted cost~\eqref{eq:mdp_problem} is the Lagrangian relaxation of a hard budget constraint $\mathbb{E}[\sum_{k=0}^{T-1}\gamma_k]\leq B$; varying $\lambda$ traces a finite-horizon performance--communication trade-off curve.

\section{Dynamic Programming and Optimal Policy}\label{sec:dp}

\subsection{Bellman Recursion}
Define the finite-horizon value functions
\begin{equation}
V_k(P)
:=
\min_{\pi_{k:T-1}}
\mathbb{E}\!\left[
\sum_{t=k}^{T-1} \big(\mathrm{tr}(W P_t) + \lambda\,\gamma_t\big)
\ \big|\ P_k=P
\right],
\label{eq:Vk_def}
\end{equation}
with terminal condition $V_T(P)=0$.
Then, for $k=T-1,\ldots,0$,
\begin{align}
V_k(P)
&=
\min_{\gamma\in\{0,1\}}
\Big\{
\mathrm{tr}(W P) + \lambda\,\gamma
 + \mathbb{E}\!\left[V_{k+1}(P_{k+1}) \mid P_k=P,\gamma_k=\gamma\right]
\Big\}
\label{eq:bellman_general}\\
&=
\min_{\gamma\in\{0,1\}}
\Big\{
\mathrm{tr}(W P) + \lambda\,\gamma
 + (1-p\gamma)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
 + p\gamma\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\Big\}.
\label{eq:bellman_explicit}
\end{align}

\subsection{Optimal Scheduling Rule as a Value-of-Information Test}\label{sec:voi}
For a given $V_{k+1}$, define the $Q$-functions
\begin{align}
Q_k(P,0) &:= \mathrm{tr}(W P) + V_{k+1}\!\big(\mathcal{T}(P)\big),\label{eq:Q0}\\
Q_k(P,1) &:= \mathrm{tr}(W P) + \lambda
 + (1-p)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
 + p\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big).
\label{eq:Q1}
\end{align}
The optimal policy at time $k$ is
\begin{equation}
\gamma_k^\star(P) = \arg\min_{\gamma\in\{0,1\}} Q_k(P,\gamma).
\label{eq:opt_policy}
\end{equation}
Equivalently, $\gamma_k^\star(P)=1$ if and only if
\begin{equation}
\lambda
\;<\;
p\Big(
V_{k+1}\!\big(\mathcal{T}(P)\big)
-V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\Big).
\label{eq:voi_test}
\end{equation}
The right-hand side is the \emph{expected value of information} from attempting a transmission at time $k$: it is the probability of delivery times the reduction in next-step cost-to-go achieved by a successful measurement update.
Equation~\eqref{eq:voi_test} provides a direct \emph{function} for computing the optimal binary decision $\gamma_k^\star$ once the value function is available.
Equivalently,
\begin{equation}
\gamma_k^\star(P)
=
\mathbf{1}\!\left\{
p\Big(
V_{k+1}\!\big(\mathcal{T}(P)\big)
-V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\Big)>\lambda
\right\}.
\label{eq:gamma_indicator}
\end{equation}

\section{Computing the Oracle Policy Offline}\label{sec:compute}
The state $P_k$ is continuous and matrix-valued, so exact dynamic programming is generally intractable except for very small problems.
To obtain an oracle benchmark for numerical studies (and a concrete object to implement), we approximate the Bellman recursion on a finite grid.

\subsection{Grid-Based Backward Induction}
Let $\mathcal{G}=\{z^1,\ldots,z^N\}$ be a finite grid of \emph{representative} covariance states.
For each representative covariance $z^i$, compute the two successors
\begin{equation}
z^{i,\mathrm{pred}} := \mathcal{T}(z^i),\qquad z^{i,\mathrm{upd}} := \mathcal{M}(\mathcal{T}(z^i)).
\label{eq:two_succ}
\end{equation}
Map each successor to a grid index (e.g., nearest-neighbor or interpolation), yielding indices $\mathrm{idx}(\mathrm{pred})$ and $\mathrm{idx}(\mathrm{upd})$.
In particular, given a feature map $\phi(\cdot)$, a convenient choice is nearest-neighbor assignment in feature space,
\begin{equation}
\mathrm{idx}(P)
:=
\arg\min_{j\in\{1,\ldots,N\}}
\ \|\phi(P)-\phi(z^j)\|_2.
\label{eq:idx_def}
\end{equation}
Backward induction then computes an approximate value table $\hat{V}_k(i)\approx V_k(z^i)$ and an action table $\hat{\gamma}_k(i)\in\{0,1\}$ by evaluating~\eqref{eq:Q0}--\eqref{eq:Q1} on the grid:
\begin{enumerate}
\item Initialize $\hat{V}_T(i)=0$ for all $i\in\{1,\ldots,N\}$.
\item For $k=T-1,\ldots,0$ and each grid point $z^i$, compute
\begin{align}
v_0 &:= \mathrm{tr}(W z^i) + \hat{V}_{k+1}\!\big(\mathrm{idx}(z^{i,\mathrm{pred}})\big), \label{eq:v0_grid}\\
v_1 &:= \mathrm{tr}(W z^i) + \lambda
 + (1-p)\,\hat{V}_{k+1}\!\big(\mathrm{idx}(z^{i,\mathrm{pred}})\big)
 + p\,\hat{V}_{k+1}\!\big(\mathrm{idx}(z^{i,\mathrm{upd}})\big).\label{eq:v1_grid}
\end{align}
\item Set $\hat{\gamma}_k(i)=1$ if $v_1<v_0$ (else $\hat{\gamma}_k(i)=0$), and set $\hat{V}_k(i)=\min\{v_0,v_1\}$.
\end{enumerate}
Online execution reduces to: (i) track $P_k$ via~\eqref{eq:P_update}, (ii) map $P_k$ to a grid index $i$, and (iii) apply $\gamma_k=\hat{\gamma}_k(i)$.
The comparison $v_1<v_0$ is the grid-level analogue of~\eqref{eq:voi_test}, with $\hat{V}_{k+1}$ in place of $V_{k+1}$.

\subsection{Trace-Only Surrogate (1D Baseline)}\label{sec:trace_surrogate}
A simple approximation replaces the matrix state $P$ by the scalar
\begin{equation}
s := \mathrm{tr}(P).
\label{eq:trace_surrogate}
\end{equation}
To obtain a closed one-dimensional recursion, we approximate $P$ as isotropic, $P\approx (s/n)I$, and propagate $(s^{\mathrm{pred}},s^{\mathrm{upd}})$ via a Kalman predict--update step on $(s/n)I$.
Discretizing $s$ on a 1D grid yields a fast finite-horizon DP and a policy table $\gamma_k^\star(s)$.
This surrogate is useful for sweeping trade-off curves, but it cannot distinguish covariances with identical trace and different anisotropy.

\subsection{Two-Feature Discretization (Trace + Logdet)}\label{sec:feature_disc}
To better capture covariance \emph{shape} while keeping the dynamic program tractable, we grid low-dimensional features of $P$ rather than the full matrix.
A simple and effective choice is
\begin{equation}
\phi(P) :=
\begin{bmatrix}
\mathrm{tr}(P)\\
\log\det(P+\varepsilon I)
\end{bmatrix},
\label{eq:features}
\end{equation}
which retains a scalar measure of total uncertainty ($\mathrm{tr}(P)$) and a measure sensitive to covariance shape ($\log\det$).
The grid $\mathcal{G}$ is built by sampling covariances reachable under random scheduling and selecting a representative covariance for each feature-cell.
Concretely, we (i) generate a pool of reachable covariances by rolling out random transmit decisions, (ii) create a rectangular grid over the observed feature ranges, (iii) assign each sample to a feature-cell, and (iv) store in each cell a representative covariance (e.g., the sample closest to the cell center), filling empty cells by nearest-neighbor propagation.
This yields a low-dimensional approximation that can distinguish covariances with similar trace but different anisotropy, while keeping the dynamic program tractable over a finite horizon.

\section{Numerical Studies}\label{sec:results}
We illustrate the finite-horizon benchmark and the resulting policy structure on representative remote-estimation experiments.
We report three metrics over a horizon $T$: (i) estimation cost $J_P:=\mathbb{E}[\sum_{k=0}^{T-1}\mathrm{tr}(P_k)]$, (ii) communication usage $J_C:=\mathbb{E}[\sum_{k=0}^{T-1}\gamma_k]$, and (iii) a closed-loop proxy $J_X:=\mathbb{E}[\sum_{k=0}^{T-1}\|x_k\|_2^2]$ under certainty-equivalent control~\eqref{eq:ce_control}.
The oracle policy is computed offline via the grid-based backward induction in Section~\ref{sec:compute}.

\subsection{Experimental Setup}
Unless otherwise stated, we use a planar double-integrator plant with sampling time $T_s=0.1$\,s and full-state sensing ($C=I$).
Process and measurement noise are Gaussian with standard deviations $(\sigma_w,\sigma_v)$, and packet delivery follows the i.i.d.\ model~\eqref{eq:drop} with success probability $p$.
We compare three schedulers:
(i) \textbf{DP-trace}, the trace-only finite-horizon DP from Section~\ref{sec:trace_surrogate};
(ii) \textbf{ET}, an event-triggered threshold rule $\gamma_k=\mathbf{1}\{\mathrm{tr}(P_k)>\delta\}$; and
(iii) \textbf{PER}, a periodic scheduler that transmits every $M$ steps.
All curves are obtained by Monte Carlo averaging over independent noise and packet-drop realizations.

\subsection{Finite-Horizon Trade-Off Curves}
Fig.~\ref{fig:tradeoff} reports finite-horizon trade-off curves obtained by sweeping $\lambda$ in~\eqref{eq:mdp_problem}.
The DP-trace benchmark achieves the best estimation and closed-loop costs among the compared low-complexity schedulers for a given communication level.
Importantly, the DP curve provides an actionable reference: it quantifies how far periodic and threshold-based policies lie from a finite-horizon dynamic-programming baseline at matched average communication.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_A_tradeoff_curves.pdf}
\caption{Finite-horizon performance--communication trade-offs obtained by sweeping $\lambda$.
Left: estimation cost $J_P$ vs.\ communication usage $J_C$.
Right: closed-loop proxy $J_X$ vs.\ $J_C$.
}
\label{fig:tradeoff}
\end{figure*}

\subsection{Trace Is Not Enough: A Counterexample}\label{sec:trace_not_enough}
The trace surrogate in Section~\ref{sec:trace_surrogate} can be strictly suboptimal because $\mathrm{tr}(P)$ discards directional information.
To illustrate this, we consider a $2$D anisotropic system with partial observation,
$A=\begin{bmatrix}1.08&0.10\\0&0.90\end{bmatrix}$,
$B=\begin{bmatrix}0.10\\0.05\end{bmatrix}$,
$C=\begin{bmatrix}1&0.25\end{bmatrix}$,
and compare \textbf{DP-trace} against \textbf{DP-2feature}, the two-feature discretization from Section~\ref{sec:feature_disc}.
\IfFileExists{figs/fig2_gap_curve.pdf}{
Fig.~\ref{fig:gapcurve} plots the estimation-cost gap under matched communication usage, showing that enriching the state with covariance-shape information can yield nontrivial improvements over trace-only dynamic programming.
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig2_gap_curve.pdf}
\caption{Trace is not enough: performance gap between trace-only DP and a two-feature DP (trace + logdet) under matched communication usage.}
\label{fig:gapcurve}
\end{figure*}
}{}

\subsection{Grow-and-Reset Dynamics Under Drops}
Fig.~\ref{fig:timeresp} shows a typical time response of $\mathrm{tr}(P_k)$ and the corresponding transmission attempts and receptions.
Between receptions, the covariance grows according to $\mathcal{T}(\cdot)$; a successful reception triggers an update $\mathcal{M}(\cdot)$ that ``resets'' uncertainty.
This grow-and-reset pattern is the mechanism exploited by scheduling policies, and it is captured exactly by the covariance-state MDP transition~\eqref{eq:P_update}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/fig_B_time_response.pdf}
\caption{Time response illustrating covariance growth between receptions and resets upon successful updates.
Vertical markers indicate attempted transmissions and successful receptions (ACKs).
}
\label{fig:timeresp}
\end{figure}

\subsection{Sensitivity to Packet Success Probability}
Finite-horizon schedules designed for a nominal packet success probability $p$ can degrade when the channel quality changes.
Fig.~\ref{fig:sensitivity} compares \emph{no-retune} execution (deploying a nominal policy under a different $p$) against \emph{retune} execution (recomputing the policy using the true $p$).
The gap quantifies the value of adapting the scheduler to channel conditions, and highlights the role of $p$ in the value-of-information test~\eqref{eq:voi_test}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_C_sensitivity_p.pdf}
\caption{Sensitivity to packet success probability $p$.
Retuning policies to the true $p$ improves both estimation cost $J_P$ and closed-loop proxy $J_X$ relative to deploying nominal schedules without retuning.
}
\label{fig:sensitivity}
\end{figure*}

\subsection{Robustness to Model Mismatch and Bursty Losses}
Finally, Fig.~\ref{fig:robustness} evaluates robustness under (i) mismatch in the assumed i.i.d.\ drop probability and (ii) bursty drops modeled by a Gilbert--Elliott channel.
We report normalized degradation relative to nominal performance.
While mismatch degrades all schedules, covariance-aware DP policies remain competitive and provide a useful oracle reference even when modeling assumptions are violated.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_D_robustness.pdf}
\caption{Robustness under channel mismatch (left) and bursty losses (right), shown as normalized degradation relative to nominal performance.
}
\label{fig:robustness}
\end{figure*}

\section{Conclusion}\label{sec:concl}
We formulated finite-horizon transmission scheduling with acknowledgements as a covariance-state MDP and derived the corresponding finite-horizon dynamic program.
The resulting optimal policy $\gamma_k^\star(P)$ admits a value-of-information characterization and serves as an oracle benchmark for finite-horizon performance--communication trade-offs under packet drops.
Because the covariance state is continuous, we described a practical offline computation procedure based on feature gridding and backward induction.
Future work will focus on scalable approximations for higher-dimensional systems, structured policy classes with performance guarantees, and learning-based approaches under unknown channel models.
