\begin{abstract}
This paper studies finite-horizon transmission scheduling for discrete-time linear systems operating over packet-dropping channels. At each time step, a sensor decides whether to attempt transmission, while packet reception is subject to random losses. With acknowledgement signals, the controller can exactly track the estimation error covariance of its Kalman filter, which constitutes a sufficient information state for scheduling decisions under linear--Gaussian assumptions.
We formulate the scheduling problem as a finite-horizon Markov decision process (MDP) on the covariance state, with a stage cost that penalizes estimation uncertainty and communication usage. The resulting dynamic program yields an MDP-optimal scheduling policy that serves as an oracle benchmark for the best achievable finite-horizon performance--communication trade-off under stochastic packet drops. This benchmark enables a systematic evaluation of structured heuristics and reveals finite-horizon effects that are not captured by infinite-horizon formulations.
\end{abstract}


\begin{IEEEkeywords}
networked control systems, transmission scheduling, Markov decision processes, packet drops, finite-horizon optimization
\end{IEEEkeywords}


\section{Introduction}

Wireless communication plays a central role in modern networked and cyber--physical systems.
In many applications, including unmanned systems and remote sensing, measurements must be delivered to a controller over unreliable communication links.
Bandwidth limitations and packet drops make periodic transmission inefficient or even infeasible, motivating the design of transmission scheduling strategies that judiciously allocate communication resources.

A large body of prior work has studied transmission scheduling and remote estimation over packet-dropping channels, often focusing on asymptotic stability or infinite-horizon average performance.
However, many practical scenarios are inherently finite-horizon, driven by mission-level objectives, energy budgets, or transient performance requirements.
In such settings, communication decisions must be optimized over a prescribed time window, and the resulting trade-offs can differ fundamentally from their infinite-horizon counterparts.

This paper studies finite-horizon transmission scheduling for discrete-time linear systems under packet drops.
We consider a setting in which the controller applies certainty-equivalent feedback and maintains a Kalman filter driven by intermittently received measurements.
Acknowledgement signals reveal whether a transmission attempt succeeds, allowing the controller to track the evolution of the estimation error covariance exactly.
Under linear--Gaussian assumptions, this covariance constitutes a sufficient information state for communication decisions.

Building on this observation, we formulate finite-horizon transmission scheduling as a Markov decision process (MDP) defined on the covariance state.
The scheduling action is binary, corresponding to whether a transmission is attempted, and the stage cost penalizes both estimation uncertainty and communication usage.
Solving the associated dynamic program yields an MDP-optimal scheduling policy, which we interpret as an oracle benchmark for the best achievable finite-horizon performance--communication trade-off under stochastic packet losses.

The goal of this paper is not to propose a deployable scheduling algorithm, but rather to establish a principled finite-horizon benchmark.
This benchmark provides insight into the structure of optimal scheduling decisions, quantifies achievable trade-offs, and serves as a reference point for evaluating low-complexity heuristics.

\section{Related Work}

Transmission scheduling and remote estimation over unreliable channels have been studied extensively in networked control and estimation.
Early work analyzed Kalman filtering under random packet drops and identified conditions for bounded estimation error.
Subsequent studies considered optimal sensor scheduling and communication policies using stochastic control and MDP formulations, typically under infinite-horizon average-cost or discounted-cost criteria.

Event-triggered and self-triggered strategies offer low-complexity alternatives by transmitting measurements only when certain conditions are met.
Many results establish stability and performance guarantees for such schemes, demonstrating significant communication savings.
However, these approaches often rely on static triggering rules and do not explicitly characterize finite-horizon optimality.

Finite-horizon formulations have received comparatively less attention, despite their relevance in mission-driven applications.
Moreover, the role of the estimation error covariance as an explicit information state in finite-horizon scheduling under packet drops is not always made explicit.
In contrast to prior work, this paper formulates finite-horizon transmission scheduling as a covariance-state MDP and uses the resulting dynamic program as an oracle benchmark to characterize achievable performance--communication trade-offs.

\section{System Model and Information State}

\subsection{Plant and Measurement Models}
We consider the discrete-time LTI system
\begin{equation}
x_{k+1} = A x_k + B u_k + w_k,
\label{eq:lti}
\end{equation}
where $x_k\in\mathbb{R}^n$, $u_k\in\mathbb{R}^m$, and $\{w_k\}$ is zero-mean i.i.d.\ process noise with covariance $Q\succeq 0$.
The sensor measures
\begin{equation}
y_k = C x_k + v_k,
\label{eq:meas}
\end{equation}
where $\{v_k\}$ is zero-mean i.i.d.\ measurement noise with covariance $R\succ 0$.
We assume $(A,B)$ is stabilizable and $(A,C)$ is detectable.

The controller applies a fixed certainty-equivalent feedback law
\begin{equation}
u_k = K \hat{x}_k,
\label{eq:ce_control}
\end{equation}
where $\hat{x}_k$ is the controller-side estimate and $K$ is chosen such that $A+BK$ is stable.

\subsection{Transmission Attempts, Packet Drops, and Acknowledgements}
At each time step, the sensor chooses whether to attempt transmission of $y_k$.
Let
\begin{equation}
a_k \in \{0,1\}
\label{eq:action}
\end{equation}
denote the transmission attempt decision.
Packet reception is modeled by an i.i.d.\ Bernoulli process
\begin{equation}
\eta_k \in \{0,1\},\qquad \Pr(\eta_k=1)=p.
\label{eq:drop}
\end{equation}
The controller observes acknowledgements, and hence the effective reception indicator
\begin{equation}
\gamma_k := a_k \eta_k
\label{eq:gamma}
\end{equation}
is known to the controller at time $k$.

\subsection{Intermittent Kalman Filtering}
The controller maintains a Kalman filter driven by intermittently received measurements.
Let $\hat{x}_{k|k}$ and $P_{k|k}$ denote the posterior estimate and covariance at time $k$ given information up to time $k$.
Define the prediction step
\begin{align}
\hat{x}_{k+1|k} &= A \hat{x}_{k|k} + B u_k, \label{eq:kf_pred_x}\\
P_{k+1|k} &= A P_{k|k} A^\top + Q. \label{eq:kf_pred_P}
\end{align}
When a measurement is successfully received ($\gamma_k=1$), the update is
\begin{align}
L_k &= P_{k|k-1} C^\top (C P_{k|k-1} C^\top + R)^{-1}, \label{eq:kf_gain}\\
\hat{x}_{k|k} &= \hat{x}_{k|k-1} + L_k \big(y_k - C\hat{x}_{k|k-1}\big), \label{eq:kf_up_x}\\
P_{k|k} &= (I - L_k C) P_{k|k-1}. \label{eq:kf_up_P}
\end{align}
If no measurement is received ($\gamma_k=0$), then $\hat{x}_{k|k}=\hat{x}_{k|k-1}$ and $P_{k|k}=P_{k|k-1}$.

\subsection{Covariance as an Information State}
For scheduling, it is convenient to work with the controller-side estimation error covariance
\begin{equation}
P_k := \mathbb{E}\!\left[(x_k-\hat{x}_{k|k})(x_k-\hat{x}_{k|k})^\top\right].
\label{eq:P_def}
\end{equation}
Given $(P_k,a_k)$, the next-step covariance evolves as
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{T}(P_k), & \gamma_k=0,\\[1mm]
\mathcal{M}\!\big(\mathcal{T}(P_k)\big), & \gamma_k=1,
\end{cases}
\label{eq:P_update}
\end{equation}
where the prediction and measurement-update operators are
\begin{align}
\mathcal{T}(P) &= A P A^\top + Q, \label{eq:T_op}\\
\mathcal{M}(P) &= P - P C^\top (C P C^\top + R)^{-1} C P. \label{eq:M_op}
\end{align}

\paragraph*{Remark (Sufficient Information State)}
Because acknowledgements reveal $\gamma_k$, the controller can track $P_k$ exactly via~\eqref{eq:P_update}.
Under the linear--Gaussian model, $P_k$ is a sufficient information state for transmission scheduling: the conditional distribution of the estimation error and its future evolution under any policy depend on the past only through $P_k$.

\paragraph*{Closed-loop Interpretation (optional)}
Under certainty-equivalent control~\eqref{eq:ce_control}, the actual closed-loop state evolves as
\begin{equation}
x_{k+1} = (A+BK)x_k + BK(x_k-\hat{x}_{k|k}) + w_k,
\label{eq:cl}
\end{equation}
showing that the estimation error enters as an additional input.
In this paper, we focus on scheduling through the covariance information state $P_k$ and use the finite-horizon MDP benchmark to characterize performance--communication trade-offs.


\section{Finite-Horizon Covariance-State MDP Formulation}

We formulate transmission scheduling as a finite-horizon Markov decision process (MDP) defined on the controller-side estimation error covariance.

\subsection{MDP Components}

\paragraph*{State}
The MDP state at time $k$ is the estimation error covariance
\begin{equation}
s_k := P_k \in \mathbb{S}^n_+,
\label{eq:mdp_state}
\end{equation}
where $\mathbb{S}^n_+$ denotes the cone of symmetric positive semidefinite matrices.

\paragraph*{Action}
The control action is the binary transmission attempt
\begin{equation}
a_k \in \mathcal{A} := \{0,1\}.
\label{eq:mdp_action}
\end{equation}

\paragraph*{State Transition}
Given $(P_k,a_k)$, the next-state covariance $P_{k+1}$ evolves according to
\begin{equation}
P_{k+1} = f(P_k,a_k,\eta_k),
\label{eq:mdp_transition}
\end{equation}
where $\eta_k \sim \mathrm{Bernoulli}(p)$ is independent across time, and the mapping
$f(\cdot)$ is induced by the Kalman covariance recursion~\eqref{eq:P_update}.
Explicitly,
\begin{equation}
f(P,a,\eta)=
\begin{cases}
\mathcal{T}(P), & a\eta = 0,\\[1mm]
\mathcal{M}\!\big(\mathcal{T}(P)\big), & a\eta = 1.
\end{cases}
\label{eq:f_def}
\end{equation}

Because acknowledgement signals reveal $\eta_k$, the controller observes the realized next state $P_{k+1}$ exactly.

\paragraph*{Stage Cost}
We define the stage cost as
\begin{equation}
c(P_k,a_k) := \mathrm{tr}(P_k) + \lambda a_k,
\label{eq:mdp_cost}
\end{equation}
where $\mathrm{tr}(P_k)$ penalizes estimation uncertainty and $\lambda>0$ weights communication usage.

\subsection{Finite-Horizon Optimization Problem}

Over a horizon of length $T$, the scheduling problem is
\begin{equation}
\min_{\pi}
\ \mathbb{E}^{\pi}\!\left[
\sum_{k=0}^{T}
\big(\mathrm{tr}(P_k) + \lambda a_k\big)
\right],
\quad
a_k = \pi_k(P_k),
\label{eq:mdp_problem}
\end{equation}
where $\pi=\{\pi_0,\ldots,\pi_T\}$ denotes a causal scheduling policy.

\paragraph*{Remark}
The formulation~\eqref{eq:mdp_problem} explicitly captures transient effects and time-varying trade-offs inherent to finite-horizon scheduling.
Unlike infinite-horizon formulations, the optimal policy need not be stationary and may depend explicitly on the remaining horizon.


\section{Dynamic Programming and Oracle Benchmark}

\subsection{Bellman Recursion}

Define the cost-to-go function at time $k$ as
\begin{equation}
V_k(P) :=
\min_{\pi_{k:T}}
\mathbb{E}\!\left[
\sum_{t=k}^{T}
\big(\mathrm{tr}(P_t) + \lambda a_t\big)
\ \big|\ P_k=P
\right].
\label{eq:Vk_def}
\end{equation}
The terminal cost is
\begin{equation}
V_{T+1}(P)=0.
\label{eq:terminal}
\end{equation}

The value functions satisfy the Bellman recursion
\begin{align}
V_k(P)
&=
\min_{a\in\{0,1\}}
\Big\{
\mathrm{tr}(P) + \lambda a
+ \mathbb{E}\!\left[
V_{k+1}\!\big(f(P,a,\eta)\big)
\right]
\Big\}
\label{eq:bellman_general}\\
&=
\min_{a\in\{0,1\}}
\Big\{
\mathrm{tr}(P) + \lambda a
+ (1-p_a)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
+ p_a\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\Big\},
\label{eq:bellman_explicit}
\end{align}
where $p_a := \Pr(a\eta=1)=ap$.

Equation~\eqref{eq:bellman_explicit} makes explicit the two possible covariance transitions induced by a transmission attempt and packet reception.

\subsection{MDP-Optimal Policy}

An MDP-optimal finite-horizon scheduling policy is given by
\begin{equation}
\pi_k^\star(P)
=
\arg\min_{a\in\{0,1\}}
\Big\{
\mathrm{tr}(P) + \lambda a
+ \mathbb{E}\!\left[
V_{k+1}\!\big(f(P,a,\eta)\big)
\right]
\Big\}.
\label{eq:opt_policy}
\end{equation}

In general, $\pi_k^\star$ is time-varying and depends on the full covariance matrix $P$.
Closed-form expressions are not available due to the continuous and high-dimensional state space.

\subsection{Oracle Benchmark Interpretation}

We interpret the solution of the Bellman recursion~\eqref{eq:bellman_general}--\eqref{eq:bellman_explicit} as an oracle benchmark for finite-horizon transmission scheduling.
The benchmark characterizes the best achievable performance--communication trade-off under the assumed model and information structure.

The oracle benchmark is not intended for real-time deployment.
Rather, it serves three purposes:
(i) it quantifies fundamental finite-horizon trade-offs,
(ii) it reveals structural properties of optimal scheduling decisions, and
(iii) it provides a principled reference for evaluating low-complexity heuristics.

\subsection{Numerical Approximation}

To compute the oracle benchmark numerically, we approximate the value functions using discretization or feature-based representations of the covariance state.
In the simulation studies, we employ low-dimensional feature approximations that preserve the dominant dependence of the cost on estimation uncertainty, while enabling tractable dynamic programming over the finite horizon.

\section{Numerical Studies}

This section evaluates the finite-horizon covariance-state MDP benchmark through representative numerical studies.
The goal is to reveal structural properties of optimal finite-horizon scheduling and to assess the importance of covariance information in transmission decisions.
All simulations use Monte Carlo averaging over independent realizations of process noise, measurement noise, and packet drops.

\subsection{Experiment 1: Finite-Horizon Scheduling Structure}

\paragraph*{Objective}
The first experiment investigates structural properties of the finite-horizon MDP-optimal scheduling policy.
In particular, we aim to illustrate how optimal transmission decisions depend jointly on the estimation uncertainty and the remaining time horizon.
Such time-varying effects are a defining feature of finite-horizon optimization and are not captured by stationary infinite-horizon policies.

\paragraph*{Setup}
We consider a discrete-time linear system of moderate dimension with a stabilizing certainty-equivalent controller.
Packet reception follows an i.i.d.\ Bernoulli model with fixed success probability.
The scheduling horizon is finite and fixed, and the MDP-optimal policy is computed via dynamic programming using a covariance-based information state.
Unless otherwise stated, all parameters are held constant throughout the horizon.

\paragraph*{Results}
Fig.~\ref{fig:exp1_policy} illustrates the structure of the MDP-optimal scheduling policy at different time steps.
For a given level of estimation uncertainty, the optimal decision exhibits a clear dependence on the remaining time.
In particular, transmission is favored earlier in the horizon, when reductions in estimation uncertainty can influence a larger portion of the cumulative cost.
As the horizon approaches its terminal stage, the same level of uncertainty may no longer justify a transmission, reflecting the diminishing value of information near the end of the horizon.

Time-domain trajectories further highlight this effect.
Along representative sample paths, the optimal policy adapts its transmission behavior over time even when the estimation uncertainty evolves similarly.
This behavior cannot be reproduced by stationary threshold-based or myopic policies, which treat all time steps identically.

\paragraph*{Discussion}
This experiment demonstrates a fundamental finite-horizon effect: optimal scheduling decisions are inherently time-varying.
The MDP benchmark explicitly captures this dependence through backward dynamic programming.
As a result, finite-horizon scheduling cannot, in general, be reduced to a stationary threshold rule without loss of optimality.
This observation motivates the use of the MDP solution as an oracle benchmark when evaluating simpler heuristics.


\subsection{Experiment 2: Limitations of Scalar Uncertainty Summaries}

\paragraph*{Objective}
The second experiment examines whether scalar summaries of the estimation error covariance, such as the trace, are sufficient for optimal finite-horizon scheduling.
While scalar metrics are appealing due to their simplicity, they may discard directional information that is critical for decision making.
This experiment demonstrates that covariance-aware scheduling can strictly outperform policies based on scalar uncertainty measures, even under identical communication budgets.

\paragraph*{Setup}
We consider a two-dimensional system with anisotropic dynamics, in which uncertainty growth differs significantly across state directions.
The measurement model provides partial observability, resulting in estimation error covariances that vary not only in magnitude but also in shape.
We compare three scheduling strategies:
(i) an MDP benchmark based on covariance-aware features,
(ii) an MDP approximation that uses only the trace of the covariance as its state,
and (iii) a fixed threshold policy tuned to match communication usage.

All policies are evaluated under the same finite horizon and communication budget.

\paragraph*{Results}
Fig.~\ref{fig:exp2_gap} compares the achieved cumulative estimation cost under matched communication usage.
The trace-based MDP approximation exhibits a noticeable performance gap relative to the covariance-aware benchmark.
This gap persists across a range of operating points and becomes more pronounced when uncertainty concentrates along dynamically critical directions.

To illustrate the underlying mechanism, Fig.~\ref{fig:exp2_scatter} shows sampled covariance states with identical trace values but different scheduling decisions under the covariance-aware MDP.
In these cases, the trace alone fails to capture the risk associated with dominant eigen-directions of the covariance, leading to suboptimal transmission decisions.

\paragraph*{Discussion}
This experiment demonstrates that scalar uncertainty measures are insufficient for finite-horizon scheduling in general.
Two covariance matrices with identical trace can have markedly different implications for future estimation performance and control cost.
By explicitly incorporating covariance information into the MDP state, the proposed benchmark captures these distinctions and achieves strictly better performance.
These results justify the covariance-state formulation and highlight its importance for principled finite-horizon scheduling.

\subsection{Experiment 3: Scheduling under Hard Communication Budgets}

\paragraph*{Objective}
The third experiment considers finite-horizon scheduling under explicit communication budget constraints.
In many practical scenarios, communication resources are specified as a hard budget over the mission duration, rather than through a soft penalty parameter.
This experiment examines how the finite-horizon MDP benchmark allocates limited communication opportunities over time and compares it with intuitive budget-aware heuristics.

\paragraph*{Setup}
We impose a hard constraint on the total number of transmission attempts over the horizon:
\[
\sum_{k=0}^{T} a_k \le B,
\]
where $B$ denotes the available communication budget.
The scheduling problem is reformulated as a finite-horizon MDP with an augmented state that tracks the remaining budget.
The objective is to minimize the cumulative estimation uncertainty subject to this constraint.

We compare the budget-aware MDP benchmark with two baseline strategies:
(i) a periodic allocation that spreads transmissions uniformly over the horizon, and
(ii) a greedy heuristic that expends budget whenever the current estimation uncertainty exceeds a fixed threshold.
All methods are evaluated under identical system and channel conditions.

\paragraph*{Results}
Fig.~\ref{fig:exp3_budget} reports the achieved estimation cost as a function of the available budget.
The MDP benchmark consistently outperforms both baselines, particularly in low-budget regimes.
Notably, the optimal policy does not distribute transmissions uniformly.
Instead, it allocates communication opportunistically based on both the current uncertainty and the remaining time horizon.

Time-domain inspection reveals a characteristic finite-horizon pattern.
When budget is scarce, the MDP policy prioritizes early transmissions that reduce uncertainty growth over a longer remaining horizon, while preserving budget for later stages when uncertainty becomes critical.
Such behavior cannot be reproduced by uniform or myopic budget allocation rules.

\paragraph*{Discussion}
This experiment highlights the importance of finite-horizon reasoning under hard communication constraints.
The budget-aware MDP benchmark explicitly captures the trade-off between immediate uncertainty reduction and future communication opportunities.
These results further reinforce the role of the finite-horizon MDP as a principled oracle benchmark for mission-level scheduling problems.


\subsection{Experiment 4: Robustness and Regret under Model Mismatch}

\paragraph*{Objective}
The final experiment evaluates the robustness of the finite-horizon MDP benchmark under model mismatch and correlated packet losses.
Rather than reporting absolute performance degradation, we adopt a regret-based perspective that quantifies the opportunity loss relative to an oracle policy designed for the true environment.

\paragraph*{Setup}
Policies are designed under nominal assumptions on packet success probability and noise statistics.
They are then evaluated under perturbed conditions, including:
(i) packet success probabilities different from the nominal design value, and
(ii) bursty packet losses modeled by a two-state Gilbert--Elliott channel with identical average success rate but varying temporal correlation.

For each perturbed environment, we compute a reference oracle policy by re-solving the finite-horizon MDP using the true model parameters.
The regret of a nominal policy is defined as the difference between its achieved cost and that of the oracle designed for the true environment.

\paragraph*{Results}
Fig.~\ref{fig:exp4_regret} reports the regret as a function of mismatch severity.
Under mild mismatch, the regret remains small, indicating that the nominal MDP benchmark degrades gracefully.
As packet losses become more bursty, regret increases due to extended communication outages that cannot be anticipated by a policy designed under independent losses.

Importantly, covariance-aware policies exhibit significantly lower regret than scalar-based or heuristic baselines across all tested scenarios.
This demonstrates that richer state representations improve robustness to modeling errors and temporal correlation in packet drops.

\paragraph*{Discussion}
This experiment highlights the value of the MDP benchmark beyond nominal optimality.
By quantifying regret relative to an oracle, we obtain a meaningful measure of robustness that directly reflects lost performance due to mismatch.
The results show that covariance-aware finite-horizon scheduling retains its advantage even when the underlying assumptions are violated, further justifying its use as a benchmark.


\section{Conclusion}

This paper formulated finite-horizon transmission scheduling under packet drops as a covariance-state Markov decision process.
By exploiting acknowledgement signals and the Kalman filter structure, the estimation error covariance was shown to constitute a sufficient information state.
The resulting dynamic program yields an oracle benchmark that characterizes the best achievable finite-horizon performance--communication trade-off.
This benchmark provides insight into the structure of optimal scheduling decisions and offers a principled reference for the evaluation of low-complexity heuristics.
Future work will focus on scalable approximations and learning-based methods for high-dimensional systems.

