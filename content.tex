\begin{abstract}
Finite-horizon networked control tasks in agricultural unmanned aerial vehicles (UAVs) require explicit and time-dependent trade-offs between estimation accuracy and communication usage due to limited mission durations and unreliable wireless links.
In such networked estimation problems, intermittent observations fundamentally alter Kalman filtering behavior and induce nontrivial scheduling effects. We propose a finite-horizon transmission scheduling framework for discrete-time linear--Gaussian systems operating over packet-dropping channels with acknowledgements (ACKs).
By exploiting ACKs, the controller can track the Kalman estimation error covariance exactly, yielding a fully observed covariance-state Markov decision process (MDP) for communication decisions.
Within this framework, we derive the finite-horizon Bellman recursion and characterize the optimal, generally time-varying scheduling policy via a value-of-information inequality.
Although the covariance state is continuous and matrix-valued, we show how to compute a practical oracle benchmark through offline dynamic programming based on discretized covariance features.
Numerical studies motivated by agricultural UAV scenarios reveal pronounced finite-horizon effects and demonstrate that incorporating covariance-shape information can significantly outperform trace-only surrogates at matched communication usage.
\end{abstract}

\begin{IEEEkeywords}
networked control systems, remote estimation, transmission scheduling, Markov decision processes, packet drops, finite-horizon optimization
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
Wireless sensing and actuation links are fundamental components of modern cyber--physical systems.
In many applications, sensor measurements must be delivered to a controller over shared and unreliable wireless channels, making communication a scarce and performance-critical resource.
Classical results have shown that packet drops fundamentally reshape Kalman filtering and estimation performance, even in linear--Gaussian systems \cite{sinopoli2004kalman,anderson2005optimal}.
When communication is costly or unreliable, deciding \emph{when} to transmit measurements becomes a central design problem.

This challenge has motivated extensive work on remote state estimation and transmission scheduling, where communication decisions are optimized jointly with estimation objectives \cite{lipsa2011remote,soleymani2021feedback}.
Most existing formulations, however, emphasize asymptotic stability or infinite-horizon average performance, often yielding stationary policies.
In contrast, many practical scenarios are mission-driven and inherently finite-horizon: a robot must complete a task within a limited time window, an inspection drone operates under a fixed battery budget, or a sensor network must finish a sweep using a finite number of transmissions.
In such settings, the \emph{remaining time} matters explicitly, and optimal scheduling decisions are generally time-varying.

This paper focuses on finite-horizon transmission scheduling for linear systems over packet-dropping channels with acknowledgements (ACKs).
Under linear--Gaussian assumptions, ACKs allow the controller to track the Kalman estimation error covariance exactly, eliminating partial observability at the controller side.
We exploit this property to formulate scheduling as a finite-horizon MDP on the covariance information state.
Unlike event-triggered heuristics or stationary scheduling rules, the resulting optimal policy is generally nonstationary and reflects the diminishing value of information as the horizon approaches.

The objective of this paper is not to propose a low-complexity deployable scheduler, but rather to develop a principled finite-horizon \emph{oracle benchmark}.
This benchmark provides a computationally explicit reference for studying performance--communication trade-offs and for evaluating structured heuristics such as event-triggered or periodic policies.
By revealing how optimal scheduling depends on both time and covariance geometry, the proposed framework clarifies fundamental finite-horizon effects that are invisible under infinite-horizon or steady-state analyses.



\section{Related Work}\label{sec:related}
Remote estimation over unreliable communication channels is a classical topic in networked control.
Early foundational results established that intermittent observations fundamentally alter Kalman filtering behavior and can lead to instability or unbounded error growth \cite{sinopoli2004kalman}.
Subsequent work studied optimal filtering and estimation performance under communication constraints and packet losses, primarily in infinite-horizon settings \cite{anderson2005optimal,lipsa2011remote}.

More recent research has explored optimal scheduling and encoding strategies over noisy, delayed, or lossy channels, including equilibrium characterizations and encoder--decoder synthesis \cite{soleymani2021feedback,soleymani2023state}.
Parallel to these developments, event-triggered and self-triggered strategies have emerged as low-complexity alternatives for reducing communication, with analyses typically focused on stability, steady-state performance, or asymptotic bounds \cite{kishida2018event,sun2022event,zhong2023event,deng2023event}.
Hybrid time- and event-triggered schemes and sensor scheduling formulations have also been proposed under limited communication resources \cite{ni2023sensor,cheng2024event}.

Learning-based and MDP-based approaches have recently been investigated for transmission scheduling and remote estimation, including reinforcement-learning formulations and value-of-information perspectives \cite{jia2024optimal,soleymani2024consistency,tzortzis2025remote}.
These works typically emphasize long-run optimality, stationary policies, or stochastic equilibria, and often treat finite horizons only implicitly.

In contrast, finite-horizon scheduling with explicit acknowledgement feedback has received comparatively less attention, despite its relevance in mission-driven applications.
This paper leverages ACKs to obtain a fully observed covariance information state, formulates a finite-horizon covariance-state MDP, and uses the resulting dynamic program as an oracle benchmark for performance--communication trade-off analysis.
Unlike infinite-horizon or steady-state treatments, the proposed framework explicitly captures time-dependent scheduling effects and highlights the role of covariance geometry beyond trace-based surrogates.


\section{System Model and Covariance Dynamics}
\label{sec:model}

We consider a discrete-time linear time-invariant (LTI) system
\begin{equation}
x_{k+1} = A x_k + B u_k + w_k,
\label{eq:lti}
\end{equation}
where $x_k\in\mathbb{R}^n$ denotes the system state and $u_k\in\mathbb{R}^m$ denotes the control input.
The process noise $\{w_k\}$ is assumed to be zero-mean i.i.d.\ with covariance $Q\succeq 0$.
The sensor measurement model is given by
\begin{equation}
y_k = C x_k + v_k,
\label{eq:meas}
\end{equation}
where $\{v_k\}$ is zero-mean i.i.d.\ measurement noise with covariance $R\succ 0$.
We assume that the pair $(A,B)$ is stabilizable and $(A,C)$ is detectable.

\subsection{Intermittent Communication and Control Architecture}

At each time step $k$, a scheduler decides whether to request transmission of the current sensor measurement.
This decision is represented by the binary variable
\begin{equation}
\gamma_k \in \{0,1\},
\end{equation}
where $\gamma_k=1$ indicates that a transmission is attempted.
Packet delivery over the communication channel is subject to i.i.d.\ Bernoulli losses,
\begin{equation}
\eta_k \in \{0,1\}, \qquad \Pr(\eta_k=1)=p,
\end{equation}
which are independent of the process and measurement noises $(w_k,v_k)$.
The effective reception indicator is therefore
\begin{equation}
\delta_k := \gamma_k \eta_k.
\end{equation}
A measurement update occurs if and only if $\delta_k=1$.
We assume that acknowledgements are available, so that $\delta_k$ is revealed to the controller before the next scheduling decision.

The controller applies a fixed certainty-equivalent state-feedback control law
\begin{equation}
u_k = -K \hat{x}_k,
\end{equation}
where $\hat{x}_k$ denotes the controller-side state estimate and $K$ is chosen such that $A-BK$ is stable.
Under this architecture, communication decisions influence closed-loop behavior solely through their impact on estimation quality.

\subsection{Controller-Side Estimation and Covariance Evolution}

The controller maintains a Kalman filter driven by intermittently received measurements.
Let
\begin{equation}
P_k := \mathbb{E}\!\left[(x_k-\hat{x}_k)(x_k-\hat{x}_k)^\top\right]
\end{equation}
denote the estimation error covariance at time $k$.
Define the prediction and update mappings
\begin{align}
\mathcal{T}(P) &:= A P A^\top + Q, \\
\mathcal{M}(P) &:= P - P C^\top (CPC^\top + R)^{-1} C P .
\end{align}

Starting from $P_k$, the covariance evolves according to
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{T}(P_k), & \delta_k=0,\\
\mathcal{M}\!\big(\mathcal{T}(P_k)\big), & \delta_k=1,
\end{cases}
\label{eq:P_update}
\end{equation}
which captures the characteristic \emph{grow-and-reset} behavior induced by intermittent communication.

\subsection{Covariance as an Information State}

Because acknowledgements reveal $\delta_k$, the controller can track the covariance $P_k$ exactly via~\eqref{eq:P_update}.
Under the linear--Gaussian assumption, the conditional distribution of future estimation errors depends on the past only through the current covariance.
Consequently, $P_k$ constitutes a sufficient information state for transmission scheduling, and the scheduling problem can be formulated as a fully observed Markov decision process (MDP) with continuous covariance state.

\section{Finite-Horizon Covariance-State MDP}
\label{sec:mdp}

We now formulate finite-horizon transmission scheduling as an MDP defined over the covariance state.

\subsection{MDP Components}

\paragraph*{State}
$s_k := P_k \in \mathbb{S}_+^n$, the estimation error covariance.

\paragraph*{Action}
$\gamma_k\in\{0,1\}$, indicating whether a transmission is attempted.

\paragraph*{Transition}
If $\gamma_k=0$, then $P_{k+1}=\mathcal{T}(P_k)$.
If $\gamma_k=1$, then
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{M}(\mathcal{T}(P_k)), & \text{w.p. } p,\\
\mathcal{T}(P_k), & \text{w.p. } 1-p .
\end{cases}
\end{equation}

\paragraph*{Stage cost}
We define the per-stage cost as
\begin{equation}
c(P_k,\gamma_k) := \mathrm{tr}(W P_k) + \lambda\,\gamma_k,
\end{equation}
where $W\succeq 0$ weights estimation uncertainty and $\lambda>0$ penalizes communication usage.
Unless stated otherwise, we set $W=I$.

\paragraph*{Scope of the objective}
This cost formulation isolates the effect of communication decisions on estimation quality and enables a tractable covariance-state MDP formulation.
Although estimation quality directly influences closed-loop performance under certainty-equivalent control, incorporating a full finite-horizon LQG cost would require augmenting the information state and would substantially alter the problem structure.
Such extensions are beyond the scope of this paper.

\subsection{Finite-Horizon Optimization Problem}

Fix a horizon $T$ and an initial covariance $P_0$.
A causal scheduling policy is a sequence $\pi=\{\pi_0,\ldots,\pi_{T-1}\}$ of measurable mappings
\begin{equation}
\gamma_k = \pi_k(P_k).
\end{equation}
The finite-horizon scheduling problem is given by
\begin{equation}
\min_{\pi}\ 
\mathbb{E}^{\pi}\!\left[
\sum_{k=0}^{T-1} \big(\mathrm{tr}(W P_k) + \lambda\,\gamma_k\big)
\right],
\label{eq:mdp_problem}
\end{equation}
subject to the covariance dynamics~\eqref{eq:P_update}.

The weighted objective~\eqref{eq:mdp_problem} can be interpreted as a Lagrangian relaxation of a hard communication budget constraint.
Varying $\lambda$ therefore traces a finite-horizon performance--communication trade-off curve.

\section{Dynamic Programming and Oracle Policy Structure}
\label{sec:dp}

\subsection{Bellman Recursion}

Let $V_T(P)=0$.
For $k=T-1,\ldots,0$, define the value function
\begin{equation}
V_k(P)
:=
\min_{\pi_{k:T-1}}
\mathbb{E}\!\left[
\sum_{t=k}^{T-1} \big(\mathrm{tr}(W P_t) + \lambda\,\gamma_t\big)
\ \big|\ P_k=P
\right].
\end{equation}
Standard backward induction yields the Bellman recursion
\begin{equation}
\begin{aligned}
V_k(P)
=
\min_{\gamma\in\{0,1\}}
\big\{
&\mathrm{tr}(W P) + \lambda\,\gamma \\
&+ (1-p\gamma)\,V_{k+1}\!\big(\mathcal{T}(P)\big) \\
&+ p\gamma\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\big\}.
\end{aligned}
\label{eq:bellman}
\end{equation}
Due to the finite horizon, the value functions $\{V_k\}$ are time-dependent, and the optimal policy is generally nonstationary.

\subsection{Value-of-Information Scheduling Rule}

For a given $V_{k+1}$, define the action-value functions
\begin{align}
Q_k(P,0)
&:= \mathrm{tr}(W P) + V_{k+1}\!\big(\mathcal{T}(P)\big), \\
Q_k(P,1)
&:= Q_k(P,0) + \lambda \nonumber\\
&\quad -\, p\!\big[
V_{k+1}\!\big(\mathcal{T}(P)\big)
- V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\big].
\end{align}

The optimal action is given by $\gamma_k^\star(P)=\arg\min_{\gamma\in\{0,1\}} Q_k(P,\gamma)$.
The final term represents the expected \emph{value of information} obtained by attempting a transmission: the probability of successful delivery multiplied by the resulting reduction in cost-to-go.
This oracle policy serves as a finite-horizon benchmark for evaluating structured event-triggered scheduling heuristics.

\section{Offline Computation of the Oracle Benchmark}
\label{sec:compute}

The covariance state is continuous and matrix-valued, rendering exact dynamic programming intractable except for very small systems.
To obtain a computable oracle benchmark, we approximate the Bellman recursion over a finite set of representative covariance states.

\subsection{Grid-Based Backward Induction}

Let $\mathcal{G}=\{z^1,\ldots,z^N\}$ denote a finite set of representative covariances.
We approximate the value functions by a table $\hat{V}_k(i)\approx V_k(z^i)$ and compute an associated policy table $\hat{\gamma}_k(i)\in\{0,1\}$.
For each $z^i\in\mathcal{G}$, define the successor covariances
\begin{equation}
z^{i,\mathrm{pred}} := \mathcal{T}(z^i),\qquad
z^{i,\mathrm{upd}} := \mathcal{M}(\mathcal{T}(z^i)).
\end{equation}
A covariance $P$ is mapped to the grid via nearest-neighbor assignment in feature space,
\begin{equation}
\mathrm{idx}(P)
:=
\arg\min_{j\in\{1,\ldots,N\}}
\ \|\phi(P)-\phi(z^j)\|_2 .
\end{equation}

\subsection{Trace-Only Surrogate (DP-trace)}
\label{sec:trace_surrogate}

The simplest approximation replaces the matrix-valued state $P_k$ by the scalar
\begin{equation}
s_k := \mathrm{tr}(P_k).
\end{equation}
Assuming isotropic uncertainty, we approximate $P_k \approx (s_k/n)I$, propagate one Kalman step from $(s_k/n)I$ to obtain the scalars $(s_k^{\mathrm{pred}},s_k^{\mathrm{upd}})$, and perform finite-horizon dynamic programming on a one-dimensional grid over $s$.
This surrogate is computationally efficient and suitable for sweeping performance-communication trade-off curves, but it cannot distinguish covariances with identical trace and different anisotropy.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figs/fig_A_tradeoff_curves.pdf}
\caption{Finite-horizon trade-offs and benchmark gaps.
Top: estimation cost $J_P$ versus communication usage $J_C$ for \textbf{DP-trace}, \textbf{ET}, and \textbf{PER}.
Bottom: estimation-cost gap $\Delta J_P$ relative to the \textbf{DP-trace} benchmark under matched $J_C$ (lower is better).}
\label{fig:tradeoff}
\end{figure}

\subsection{Two-Feature Discretization (DP-2feature)}
\label{sec:feature_disc}

To better capture covariance geometry while maintaining tractability, we discretize low-dimensional features of $P$ rather than the full matrix.
We adopt the feature map
\begin{equation}
\phi(P)=
\begin{bmatrix}
\mathrm{tr}(P)\\
\log\det(P+\varepsilon I)
\end{bmatrix},
\end{equation}
which combines a measure of total uncertainty with a shape-sensitive term.
The grid $\mathcal{G}$ is constructed by sampling covariances reachable under random scheduling, binning samples in feature space, and selecting a representative covariance for each feature cell, with empty cells filled via nearest-neighbor propagation.
This approximation balances fidelity and computational complexity, enabling offline computation of a strong finite-horizon oracle benchmark.

\section{Evaluation}
\label{sec:results}




This section evaluates the proposed finite-horizon covariance-state MDP benchmark and uses it as a reference to assess representative heuristic schedulers.
We focus on the fundamental finite-horizon trade-off between estimation accuracy and communication usage induced by intermittent measurements and packet drops.
The primary metrics are
\begin{equation}
J_P := \mathbb{E}\!\left[\sum_{k=0}^{T-1}\mathrm{tr}(P_k)\right],\qquad
J_C := \mathbb{E}\!\left[\sum_{k=0}^{T-1}\gamma_k\right],
\end{equation}
which quantify cumulative estimation uncertainty and total transmission attempts over the horizon, respectively.
To connect the estimation-centric benchmark to closed-loop regulation under the fixed certainty-equivalent controller~\eqref{eq:ce_control}, we additionally report the proxy metric
\begin{equation}
J_X := \mathbb{E}\!\left[\sum_{k=0}^{T-1}\|x_k\|_2^2\right],
\end{equation}
which is \emph{not} optimized by the covariance-state MDP in~\eqref{eq:mdp_problem} but provides an interpretable measure of state-regulation quality.
All DP-based policies are computed offline via the discretized backward-induction procedure in Section~\ref{sec:compute}.
All reported curves are obtained by Monte Carlo averaging over independent realizations of process/measurement noise and packet-drop processes.

\subsection{Experimental Setup}

Unless otherwise stated, we consider a planar double-integrator plant discretized with sampling time $T_s=0.1$\,s and full-state sensing ($C=I$).
Process and measurement noises are zero-mean Gaussian with standard deviations $(\sigma_w,\sigma_v)$, and packet delivery follows the i.i.d.\ Bernoulli model~\eqref{eq:drop} with success probability $p$.
We compare the following schedulers:
\begin{itemize}
\item \textbf{DP-trace}: the trace-only finite-horizon dynamic programming policy in Section~\ref{sec:trace_surrogate}, which serves as a computationally tractable oracle benchmark;
\item \textbf{ET}: a standard event-triggered policy $\gamma_k=\mathbf{1}\{\mathrm{tr}(P_k)>\delta\}$ with threshold $\delta$ swept to obtain different trade-off points;
\item \textbf{PER}: a periodic scheduler that transmits every $M$ steps, with period $M$ swept to vary communication usage.
\end{itemize}
For the counterexample in Section~\ref{sec:trace_not_enough}, we additionally evaluate \textbf{DP-2feature} from Section~\ref{sec:feature_disc}, which enriches the discretization with covariance-shape information.
Unless otherwise stated, all methods are evaluated under identical noise statistics and identical channel parameters.

\subsection{Finite-Horizon Trade-Off Curves}
\label{subsec:tradeoff}

We first quantify the finite-horizon performance--communication trade-off and use the DP benchmark to interpret the suboptimality of heuristic schedulers.
Fig.~\ref{fig:tradeoff} reports trade-off curves obtained by sweeping the Lagrange multiplier $\lambda$ in~\eqref{eq:mdp_problem} for \textbf{DP-trace}, and sweeping the corresponding tuning parameters for \textbf{ET} (threshold $\delta$) and \textbf{PER} (period $M$).

The top panel of Fig.~\ref{fig:tradeoff} shows $J_P$ as a function of $J_C$.
Across the sweep, the DP benchmark forms the lower envelope of achievable trade-offs, reflecting its explicit optimization of the finite-horizon objective over the covariance information state.
Both heuristic baselines incur higher estimation cost at comparable communication usage, with the gap most pronounced in the intermediate regime where the scheduling choice has the largest marginal value.

To make the comparison directly interpretable, the bottom panel plots the estimation-cost gap
\begin{equation}
\Delta J_P := J_P^{\mathrm{heur}}-J_P^{\mathrm{DP}}
\end{equation}
under matched communication usage $J_C$.
This representation separates ``how much one communicates'' from ``how effectively communication is allocated over time,'' and therefore isolates the benefit of finite-horizon, time-dependent scheduling decisions captured by the DP benchmark.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figs/fig_C_sensitivity_p.pdf}
\caption{Sensitivity to packet success probability $p$.
Top: Lagrangian objective $J_\lambda = J_P + \lambda J_C$ (fixed $\lambda$).
Bottom: resulting communication usage $J_C$.
Retuning the scheduler to the true $p$ improves $J_\lambda$ relative to deploying a nominal policy under mismatch.}
\label{fig:sensitivity}
\end{figure}

\subsection{Trace Is Not Enough: A Counterexample}
\label{sec:trace_not_enough}

The trace-only surrogate in Section~\ref{sec:trace_surrogate} is attractive for offline computation, but it can be strictly suboptimal because $\mathrm{tr}(P)$ discards directional information. To illustrate when this loss of information matters, we consider a two-dimensional anisotropic system with partial observation
\[
A=\begin{bmatrix}1.08&0.10\\0&0.90\end{bmatrix},\qquad
B=\begin{bmatrix}0.10\\0.05\end{bmatrix},\qquad
C=\begin{bmatrix}1&0.25\end{bmatrix}.
\]
In this setting, two covariances with the same trace can imply different future uncertainty growth depending on anisotropy and observability directions.
Consequently, a policy that conditions only on $\mathrm{tr}(P_k)$ may allocate transmissions differently from a policy that also accounts for covariance shape.

We therefore compare \textbf{DP-trace} against \textbf{DP-2feature}, which discretizes the covariance using both trace and a shape-sensitive feature (log-det) as described in Section~\ref{sec:feature_disc}.
Fig.~\ref{fig:gapcurve} reports the estimation-cost gap under matched communication usage, demonstrating that incorporating even low-dimensional shape information can yield a nontrivial improvement when the system dynamics and sensing direction induce anisotropic uncertainty growth.

\IfFileExists{figs/fig2_gap_curve.pdf}{
\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figs/fig2_gap_curve.pdf}
\caption{Trace is not enough: performance gap between \textbf{DP-trace} and \textbf{DP-2feature} (trace + log-det) under matched communication usage.}
\label{fig:gapcurve}
\end{figure}
}{}

\subsection{Grow-and-Reset Dynamics Under Drops}
\label{subsec:growreset}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figs/fig_B_time_response.pdf}
\caption{Representative \textbf{DP-trace} rollout illustrating finite-horizon grow-and-reset behavior.
Top: covariance trace $\mathrm{tr}(P_k)$ and the induced time-varying switching boundary $s_k^\star$.
Middle: transmission attempts and successful receptions (ACKs).
Bottom: estimation error norm $\|x_k-\hat{x}_k\|_2$.}
\label{fig:timeresp}
\end{figure}

We next visualize the finite-horizon mechanism underlying the DP benchmark.
Fig.~\ref{fig:timeresp} shows a representative rollout under \textbf{DP-trace}.
The top panel plots the covariance trace $\mathrm{tr}(P_k)$ together with the induced time-varying DP switching boundary $s_k^\star$.
The resulting scheduling behavior is well-approximated by a \emph{time-dependent} threshold policy: a transmission is attempted when $\mathrm{tr}(P_k)$ exceeds $s_k^\star$.

Between successful receptions (ACKs), the covariance grows according to the prediction map $\mathcal{T}(\cdot)$.
When a packet is successfully delivered, the Kalman update $\mathcal{M}(\cdot)$ sharply reduces uncertainty, yielding the characteristic grow-and-reset pattern.
Importantly, the switching boundary increases as the horizon approaches, reflecting the diminishing value of information when fewer future stages remain.
The bottom panel further confirms that this behavior aligns with reductions in the estimation error norm $\|x_k-\hat{x}_k\|_2$ following successful updates.



\subsection{Sensitivity to Packet Success Probability}
\label{subsec:sensitivity}

Finite-horizon schedules are explicitly shaped by the channel success probability $p$ through the transition model and, for DP policies, through the value-of-information structure.
To quantify the impact of channel mismatch, Fig.~\ref{fig:sensitivity} compares:
(i) \emph{no-retune} execution, where a policy designed for a nominal $p$ is deployed under a different true channel success probability; and
(ii) \emph{retune} execution, where the policy is recomputed using the true $p$.

The top panel reports the Lagrangian objective $J_\lambda := J_P + \lambda J_C$ for a fixed $\lambda$, while the bottom panel reports the induced communication usage $J_C$.
Across the range of $p$, retuning yields consistent improvements relative to no-retune deployment, quantifying the value of adapting the scheduler to channel conditions.
Moreover, the comparison indicates that mismatch can simultaneously degrade estimation performance and alter communication usage, underscoring that $p$ influences not only reliability but also the optimal allocation of transmission attempts over a finite horizon.

\subsection{Robustness to Model Mismatch and Bursty Losses}
\label{subsec:robustness}

Finally, we evaluate robustness under deviations from the nominal i.i.d.\ drop model.
Fig.~\ref{fig:robustness} reports two stress tests.
In the top panel, we introduce mismatch in the assumed i.i.d.\ drop probability and report normalized degradation relative to the nominal design point, for both estimation performance ($J_P$) and the closed-loop proxy ($J_X$).
In the bottom panel, we consider temporally correlated (bursty) packet drops modeled by a Gilbert--Elliott channel.
To isolate the impact of burstiness beyond the marginal success rate, we keep the mean success probability fixed and vary the temporal correlation as quantified by the mean loss-burst length.

While model mismatch and burstiness inevitably degrade performance compared to the nominal i.i.d.\ design condition, the DP benchmark remains informative: it continues to provide a strong finite-horizon reference curve against which structured heuristics can be evaluated, and it reveals how far practical schedulers are from the finite-horizon oracle under realistic channel impairments.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\columnwidth]{figs/fig_D_robustness.pdf}
\caption{Robustness to non-ideal channels.
Top: normalized degradation under mismatch in the packet success probability, reported for both estimation cost ($J_P$) and regulation proxy ($J_X$), relative to the nominal i.i.d.\ setting.
Bottom: performance under bursty packet drops (Gilbert--Elliott channel) at fixed mean success probability, as a function of burstiness (mean loss-burst length).}
\label{fig:robustness}
\end{figure}


\section{Conclusion}\label{sec:concl}
We formulated finite-horizon transmission scheduling with acknowledgements as a covariance-state MDP and derived the corresponding finite-horizon dynamic program.
The resulting optimal policy $\gamma_k^\star(P)$ admits a value-of-information characterization and serves as an oracle benchmark for finite-horizon performance--communication trade-offs under packet drops.
Because the covariance state is continuous, we described a practical offline computation procedure based on feature gridding and backward induction.
Future work will focus on scalable approximations for higher-dimensional systems, structured policy classes with performance guarantees, and learning-based approaches under unknown channel models.
