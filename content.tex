\begin{abstract}
Finite-horizon networked control tasks require explicit trade-offs between estimation quality and communication usage.
We study transmission scheduling for discrete-time linear--Gaussian systems operating over packet-dropping channels with acknowledgements (ACKs).
Because ACKs reveal reception outcomes, the controller can track the Kalman estimation error covariance exactly, which yields a fully observed covariance-state Markov decision process (MDP) for communication decisions.
We derive the finite-horizon Bellman recursion and characterize the optimal time-varying scheduling policy $\gamma_k^\star(P)$ via a value-of-information inequality.
To obtain a computable benchmark despite the continuous covariance state, we describe an offline dynamic program based on discretizing low-dimensional covariance features.
Numerical studies illustrate finite-horizon effects and show that incorporating covariance-shape information can outperform trace-only surrogates at matched communication usage.
\end{abstract}

\begin{IEEEkeywords}
networked control systems, remote estimation, transmission scheduling, Markov decision processes, packet drops, finite-horizon optimization
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
Wireless sensing and actuation links are fundamental components of modern cyber--physical systems.
In many applications, sensor measurements must be delivered to a controller over shared and unreliable wireless channels.
Periodic transmission can be wasteful under bandwidth or energy constraints, while unreliable links with packet drops make open-loop scheduling brittle.
This motivates \emph{transmission scheduling}: deciding \emph{when} to transmit measurements so as to balance control/estimation performance against communication usage.

Most existing scheduling formulations emphasize asymptotic stability or infinite-horizon average performance.
However, many practical scenarios are mission-driven and inherently finite-horizon: a robot must traverse a corridor in $30$\,s, an inspection drone has a short battery window, or a sensor network must complete a sweep under a limited packet budget.
In such settings, the \emph{remaining time} matters, and optimal scheduling decisions are generally time-varying.

This paper focuses on finite-horizon scheduling for linear systems over packet-dropping channels with acknowledgements (ACKs).
Under linear--Gaussian assumptions, ACKs allow the controller to track the Kalman estimation error covariance exactly.
We exploit this property to formulate scheduling as a finite-horizon MDP on the covariance information state and to compute an MDP-optimal (oracle) policy.
The objective is not to propose a low-complexity deployable scheduler, but to provide a principled finite-horizon benchmark and a clear computational recipe that can be used to design and evaluate practical heuristics.

\paragraph*{Contributions}
\begin{itemize}
\item We formulate finite-horizon transmission scheduling with ACKs as a fully observed covariance-state MDP.
\item We derive the finite-horizon Bellman recursion and an optimal, generally time-varying policy $\gamma_k^\star(P)$ characterized by a value-of-information test.
\item We present an offline approximation procedure (feature discretization + backward induction) that yields a practical oracle benchmark for finite-horizon trade-off studies.
\end{itemize}

\section{Related Work}\label{sec:related}
Remote estimation over unreliable channels and Kalman filtering with intermittent observations are classical topics in networked control and estimation.
Many scheduling formulations use infinite-horizon discounted or average-cost criteria, often seeking stationary policies.
Event-triggered and self-triggered strategies provide low-complexity alternatives by transmitting only when a trigger condition is met, with analyses typically centered on stability or steady-state performance.

Finite-horizon scheduling has received comparatively less attention despite its relevance in mission-driven settings, and the optimal decisions are generally nonstationary because the remaining time matters.
This paper leverages acknowledgements to obtain a fully observed covariance information state, formulates a finite-horizon covariance-state MDP, and uses the resulting dynamic program as an oracle benchmark for performance--communication trade-offs.


\section{System Model and Covariance Dynamics}
\label{sec:model}

We consider a discrete-time linear time-invariant (LTI) system
\begin{equation}
x_{k+1} = A x_k + B u_k + w_k,
\label{eq:lti}
\end{equation}
where $x_k\in\mathbb{R}^n$, $u_k\in\mathbb{R}^m$, and $\{w_k\}$ is zero-mean i.i.d.\ process noise with covariance $Q\succeq 0$.
The sensor measures
\begin{equation}
y_k = C x_k + v_k,
\label{eq:meas}
\end{equation}
where $\{v_k\}$ is zero-mean i.i.d.\ measurement noise with covariance $R\succ 0$.
We assume $(A,B)$ is stabilizable and $(A,C)$ is detectable.

\subsection{Intermittent Communication and Control Architecture}

At each time step $k$, a scheduler decides whether to attempt transmission of the current measurement.
This decision is represented by a binary variable
\begin{equation}
\gamma_k \in \{0,1\},
\label{eq:gamma}
\end{equation}
where $\gamma_k=1$ requests transmission and $\gamma_k=0$ corresponds to silence.
Packet delivery is subject to random losses modeled by an i.i.d.\ Bernoulli process
\begin{equation}
\eta_k \in \{0,1\}, \qquad \Pr(\eta_k=1)=p,
\label{eq:drop}
\end{equation}
independent across time and independent of $(w_k,v_k)$.
The controller observes an acknowledgement (ACK), which reveals the effective reception indicator
\begin{equation}
\delta_k := \gamma_k \eta_k.
\label{eq:delta}
\end{equation}
Thus, a measurement update occurs if and only if $\delta_k=1$.
We assume the ACK is available to the decision maker before the next scheduling decision, so the realized update outcome is known when propagating the information state.

The controller applies a fixed certainty-equivalent state feedback law
\begin{equation}
u_k = -K \hat{x}_k,
\label{eq:ce_control}
\end{equation}
where $\hat{x}_k$ denotes the controller-side state estimate and the gain $K$ is chosen such that $A-BK$ is stable.
Throughout the paper, the control gain is held fixed and communication decisions affect closed-loop behavior only through the quality of state estimation.

\subsection{Controller-Side Estimation and Covariance Evolution}

The controller maintains a Kalman filter driven by intermittently received measurements.
Let
\begin{equation}
P_k := \mathbb{E}\!\left[(x_k-\hat{x}_k)(x_k-\hat{x}_k)^\top\right]
\label{eq:P_def}
\end{equation}
denote the estimation error covariance available to the controller at decision time~$k$.
Given $P_k$, the one-step prediction operator is
\begin{equation}
\mathcal{T}(P_k) := A P_k A^\top + Q,
\label{eq:T_op}
\end{equation}
and the corresponding measurement-update operator is
\begin{equation}
\mathcal{M}(P) := P - P C^\top (C P C^\top + R)^{-1} C P.
\label{eq:M_op}
\end{equation}

Starting from $P_k$, the controller first forms the predicted covariance $\mathcal{T}(P_k)$.
If $\delta_k=1$, a Kalman update is applied; otherwise, the estimator remains in prediction mode.
The resulting covariance satisfies the recursion
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{T}(P_k), & \delta_k=0,\\[1mm]
\mathcal{M}\!\big(\mathcal{T}(P_k)\big), & \delta_k=1.
\end{cases}
\label{eq:P_update}
\end{equation}
This grow-and-reset evolution captures how intermittent communication regulates the accumulation and contraction of estimation uncertainty.

\subsection{Covariance as an Information State}

Because acknowledgements reveal $\delta_k$, the controller can track the covariance $P_k$ exactly via~\eqref{eq:P_update}.
Under the linear--Gaussian model, the conditional distribution of future estimation errors depends on the past only through the current covariance.
As a result, $P_k$ constitutes a sufficient information state for scheduling decisions, and transmission scheduling can be formulated as a fully observed Markov decision process on the covariance state.

\section{Finite-Horizon Covariance-State MDP}
\label{sec:mdp}

We formulate transmission scheduling over a finite horizon as a Markov decision process with continuous, matrix-valued state.

\subsection{MDP Components}

\paragraph*{State}
The state at time $k$ is the estimation error covariance $s_k := P_k \in \mathbb{S}_+^n$, where $\mathbb{S}_+^n$ denotes the cone of symmetric positive semidefinite matrices.

\paragraph*{Action}
The action is the binary scheduling decision $\gamma_k\in\{0,1\}$.

\paragraph*{Transition}
Given $(P_k,\gamma_k)$, the next covariance $P_{k+1}$ evolves according to~\eqref{eq:P_update}.
Equivalently, if $\gamma_k=0$, the transition is deterministic with $P_{k+1}=\mathcal{T}(P_k)$, while if $\gamma_k=1$,
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{M}(\mathcal{T}(P_k)), & \text{with probability } p,\\
\mathcal{T}(P_k), & \text{with probability } 1-p.
\end{cases}
\label{eq:mdp_transition}
\end{equation}

\paragraph*{Stage cost}
We use the stage cost
\begin{equation}
c(P_k,\gamma_k) := \mathrm{tr}(W P_k) + \lambda\,\gamma_k,
\label{eq:stage_cost}
\end{equation}
where $W\succeq 0$ weights estimation uncertainty and $\lambda>0$ penalizes communication usage.
Unless otherwise stated, we use $W=I$, yielding $c(P,\gamma)=\mathrm{tr}(P)+\lambda\gamma$.
\paragraph*{Scope of the objective}
This choice isolates the impact of communication decisions on estimation quality and allows the finite-horizon scheduling problem to be formulated as a covariance-state MDP.
While estimation quality directly affects closed-loop behavior under certainty-equivalent control, incorporating a full finite-horizon LQG cost would require augmenting the information state and would substantially change the problem formulation, which is beyond the scope of this paper.

\subsection{Finite-Horizon Optimization Problem}

Fix a horizon $T$ and an initial covariance $P_0$.
A causal scheduling policy is a sequence $\pi=\{\pi_0,\ldots,\pi_{T-1}\}$ of measurable mappings
\begin{equation}
\gamma_k = \pi_k(P_k).
\end{equation}
The finite-horizon scheduling problem is
\begin{equation}
\min_{\pi}\ 
\mathbb{E}^{\pi}\!\left[
\sum_{k=0}^{T-1} \big(\mathrm{tr}(W P_k) + \lambda\,\gamma_k\big)
\right],
\label{eq:mdp_problem}
\end{equation}
subject to the covariance dynamics~\eqref{eq:P_update}.

The weighted cost~\eqref{eq:mdp_problem} can be interpreted as the Lagrangian relaxation of a hard communication budget constraint.
Varying $\lambda$ traces a finite-horizon performance--communication trade-off curve.

\section{Dynamic Programming and Oracle Policy Structure}
\label{sec:dp}

\subsection{Bellman Recursion}

Define the value functions
\begin{equation}
V_k(P)
:=
\min_{\pi_{k:T-1}}
\mathbb{E}\!\left[
\sum_{t=k}^{T-1} \big(\mathrm{tr}(W P_t) + \lambda\,\gamma_t\big)
\ \big|\ P_k=P
\right],
\label{eq:Vk_def}
\end{equation}
with terminal condition $V_T(P)=0$.
Backward induction yields the Bellman recursion
\begin{align}
V_k(P)
&=
\min_{\gamma\in\{0,1\}}
\Big\{
\mathrm{tr}(W P) + \lambda\,\gamma
 + (1-p\gamma)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
 + p\gamma\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\Big\}.
\label{eq:bellman}
\end{align}
Because the horizon is finite, the value functions $\{V_k\}$ depend on the time index $k$, and the optimal policy is generally time-varying (nonstationary).

\subsection{Value-of-Information Scheduling Rule}

For a given $V_{k+1}$, define
\begin{align}
Q_k(P,0) &:= \mathrm{tr}(W P) + V_{k+1}\!\big(\mathcal{T}(P)\big),\\
Q_k(P,1) &:= \mathrm{tr}(W P) + \lambda
 + (1-p)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
 + p\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big).
\end{align}
Then $\gamma_k^\star(P)=\arg\min_{\gamma\in\{0,1\}} Q_k(P,\gamma)$.
The optimal action satisfies
\begin{equation}
\gamma_k^\star(P)=1
\quad \Longleftrightarrow \quad
\lambda < p\!\left(
V_{k+1}\!\big(\mathcal{T}(P)\big)
-
V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\right).
\label{eq:voi_test}
\end{equation}
Equivalently,
\begin{equation}
\gamma_k^\star(P)
=
\mathbf{1}\!\left\{
p\!\left(
V_{k+1}\!\big(\mathcal{T}(P)\big)
-V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\right)>\lambda
\right\}.
\label{eq:gamma_star}
\end{equation}
The right-hand side represents the expected value of information from attempting a transmission: the probability of successful delivery times the reduction in cost-to-go achieved by a measurement update.
This oracle policy provides a finite-horizon benchmark for evaluating structured event-triggered heuristics.

\section{Offline Computation of the Oracle Benchmark}
\label{sec:compute}

The covariance state is continuous and matrix-valued, rendering exact dynamic programming intractable except for very small systems.
To obtain a computable oracle benchmark, we approximate the Bellman recursion on a finite set of representative covariance states.

\subsection{Grid-Based Backward Induction}

Let $\mathcal{G}=\{z^1,\ldots,z^N\}$ be a finite set of representative covariances.
We approximate the value functions by a table $\hat{V}_k(i)\approx V_k(z^i)$ and compute an associated policy table $\hat{\gamma}_k(i)\in\{0,1\}$.
For each grid point $z^i$, define the two successor covariances
\begin{equation}
z^{i,\mathrm{pred}} := \mathcal{T}(z^i),\qquad
z^{i,\mathrm{upd}} := \mathcal{M}(\mathcal{T}(z^i)).
\label{eq:grid_successors}
\end{equation}
Given a feature map $\phi(\cdot)$ (defined below), we map any covariance $P$ to a grid index by nearest-neighbor assignment in feature space,
\begin{equation}
\mathrm{idx}(P)
:=
\arg\min_{j\in\{1,\ldots,N\}}
\ \|\phi(P)-\phi(z^j)\|_2.
\label{eq:idx_def}
\end{equation}
Backward induction then evaluates the Bellman update on the grid:
\begin{enumerate}
\item Initialize $\hat{V}_T(i)=0$ for all $i\in\{1,\ldots,N\}$.
\item For $k=T-1,\ldots,0$ and each $z^i\in\mathcal{G}$, compute
\begin{align}
v_0 &:= \mathrm{tr}(W z^i) + \hat{V}_{k+1}\!\big(\mathrm{idx}(z^{i,\mathrm{pred}})\big), \label{eq:v0_grid}\\
v_1 &:= \mathrm{tr}(W z^i) + \lambda
 + (1-p)\,\hat{V}_{k+1}\!\big(\mathrm{idx}(z^{i,\mathrm{pred}})\big)
 + p\,\hat{V}_{k+1}\!\big(\mathrm{idx}(z^{i,\mathrm{upd}})\big). \label{eq:v1_grid}
\end{align}
\item Set $\hat{\gamma}_k(i)=1$ if $v_1<v_0$ (else $\hat{\gamma}_k(i)=0$), and set $\hat{V}_k(i)=\min\{v_0,v_1\}$.
\end{enumerate}
Online execution uses the exact covariance recursion~\eqref{eq:P_update} to track $P_k$, maps $P_k$ to $i=\mathrm{idx}(P_k)$, and applies $\gamma_k=\hat{\gamma}_k(i)$.

\subsection{Trace-Only Surrogate (DP-trace)}\label{sec:trace_surrogate}
The simplest discretization replaces the matrix state $P$ by the scalar
\begin{equation}
s_k := \mathrm{tr}(P_k).
\label{eq:trace_state}
\end{equation}
To obtain a closed one-dimensional recursion, we approximate $P_k$ as isotropic, $P_k \approx (s_k/n)I$, propagate one Kalman step from $(s_k/n)I$ to obtain scalars $(s^{\mathrm{pred}}_k,s^{\mathrm{upd}}_k)$, and run the finite-horizon DP on a 1D grid over $s$.
This baseline is computationally efficient and useful for sweeping trade-off curves, but it cannot distinguish covariances with identical trace and different anisotropy.

\subsection{Two-Feature Discretization (DP-2feature)}\label{sec:feature_disc}
To better capture covariance \emph{shape} while keeping the dynamic program tractable, we discretize low-dimensional features of $P$ rather than the full matrix.
We use the two-dimensional feature map
\begin{equation}
\phi(P)=
\begin{bmatrix}
\mathrm{tr}(P)\\
\log\det(P+\varepsilon I)
\end{bmatrix},
\label{eq:features}
\end{equation}
which combines a measure of total uncertainty ($\mathrm{tr}(P)$) with a shape-sensitive term ($\log\det$).
We construct $\mathcal{G}$ by sampling covariances reachable under random scheduling, binning samples by their feature values on a rectangular feature grid, and selecting a representative covariance (prototype) for each feature cell, filling empty cells by nearest-neighbor propagation.
This approximation balances fidelity and tractability, enabling the offline computation of a strong finite-horizon benchmark.

\section{Numerical Studies}\label{sec:results}
We illustrate the finite-horizon benchmark and the resulting policy structure on representative remote-estimation experiments.
The primary metrics are the estimation objective $J_P:=\mathbb{E}[\sum_{k=0}^{T-1}\mathrm{tr}(P_k)]$ and the communication usage $J_C:=\mathbb{E}[\sum_{k=0}^{T-1}\gamma_k]$.
To connect the estimation benchmark to regulation performance under the fixed certainty-equivalent controller~\eqref{eq:ce_control}, we additionally report a closed-loop proxy $J_X:=\mathbb{E}[\sum_{k=0}^{T-1}\|x_k\|_2^2]$ (not optimized by~\eqref{eq:mdp_problem}).
All DP-based policies are computed offline via the discretized backward induction procedure in Section~\ref{sec:compute}.

\subsection{Experimental Setup}
Unless otherwise stated, we use a planar double-integrator plant with sampling time $T_s=0.1$\,s and full-state sensing ($C=I$).
Process and measurement noise are Gaussian with standard deviations $(\sigma_w,\sigma_v)$, and packet delivery follows the i.i.d.\ model~\eqref{eq:drop} with success probability $p$.
We compare three schedulers:
(i) \textbf{DP-trace}, the trace-only finite-horizon DP from Section~\ref{sec:trace_surrogate};
(ii) \textbf{ET}, an event-triggered threshold rule $\gamma_k=\mathbf{1}\{\mathrm{tr}(P_k)>\delta\}$; and
(iii) \textbf{PER}, a periodic scheduler that transmits every $M$ steps.
For the counterexample in Section~\ref{sec:trace_not_enough}, we additionally evaluate \textbf{DP-2feature} from Section~\ref{sec:feature_disc}.
All curves are obtained by Monte Carlo averaging over independent noise and packet-drop realizations.

\subsection{Finite-Horizon Trade-Off Curves}
Fig.~\ref{fig:tradeoff} reports finite-horizon trade-off curves obtained by sweeping $\lambda$ in~\eqref{eq:mdp_problem}.
In our experiments, the DP-trace baseline provides a strong finite-horizon benchmark and typically improves upon periodic and threshold-based scheduling at matched communication usage.
Importantly, the DP curve provides an actionable reference: it quantifies how far simple heuristic policies lie from a finite-horizon dynamic-programming baseline over the entire trade-off range.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_A_tradeoff_curves.pdf}
\caption{Finite-horizon performance--communication trade-offs obtained by sweeping $\lambda$.
Left: estimation cost $J_P$ vs.\ communication usage $J_C$.
Right: closed-loop proxy $J_X$ vs.\ $J_C$.
}
\label{fig:tradeoff}
\end{figure*}

\subsection{Trace Is Not Enough: A Counterexample}\label{sec:trace_not_enough}
The trace surrogate in Section~\ref{sec:trace_surrogate} can be strictly suboptimal because $\mathrm{tr}(P)$ discards directional information.
To illustrate this, we consider a $2$D anisotropic system with partial observation,
$A=\begin{bmatrix}1.08&0.10\\0&0.90\end{bmatrix}$,
$B=\begin{bmatrix}0.10\\0.05\end{bmatrix}$,
$C=\begin{bmatrix}1&0.25\end{bmatrix}$,
and compare \textbf{DP-trace} against \textbf{DP-2feature}, the two-feature discretization from Section~\ref{sec:feature_disc}.
In this setting, two covariances with the same trace can lead to different future uncertainty growth depending on their anisotropy, so incorporating covariance-shape information can change the scheduling decision and improve performance at matched communication usage.
\IfFileExists{figs/fig2_gap_curve.pdf}{
Fig.~\ref{fig:gapcurve} plots the estimation-cost gap under matched communication usage, showing that enriching the state with covariance-shape information can yield nontrivial improvements over trace-only dynamic programming.
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig2_gap_curve.pdf}
\caption{Trace is not enough: performance gap between trace-only DP and a two-feature DP (trace + logdet) under matched communication usage.}
\label{fig:gapcurve}
\end{figure*}
}{}

\subsection{Grow-and-Reset Dynamics Under Drops}
Fig.~\ref{fig:timeresp} shows a typical time response of $\mathrm{tr}(P_k)$ and the corresponding transmission attempts and receptions.
Between receptions, the covariance grows according to $\mathcal{T}(\cdot)$; a successful reception triggers an update $\mathcal{M}(\cdot)$ that ``resets'' uncertainty.
This grow-and-reset pattern is the mechanism exploited by scheduling policies, and it is captured exactly by the covariance-state MDP transition~\eqref{eq:P_update}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/fig_B_time_response.pdf}
\caption{Time response illustrating covariance growth between receptions and resets upon successful updates.
Vertical markers indicate attempted transmissions and successful receptions (ACKs).
}
\label{fig:timeresp}
\end{figure}

\subsection{Sensitivity to Packet Success Probability}
Finite-horizon schedules designed for a nominal packet success probability $p$ can degrade when the channel quality changes.
Fig.~\ref{fig:sensitivity} compares \emph{no-retune} execution (deploying a nominal policy under a different $p$) against \emph{retune} execution (recomputing the policy using the true $p$).
The gap quantifies the value of adapting the scheduler to channel conditions, and highlights the role of $p$ in the value-of-information test~\eqref{eq:voi_test}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_C_sensitivity_p.pdf}
\caption{Sensitivity to packet success probability $p$.
Retuning policies to the true $p$ improves both estimation cost $J_P$ and closed-loop proxy $J_X$ relative to deploying nominal schedules without retuning.
}
\label{fig:sensitivity}
\end{figure*}

\subsection{Robustness to Model Mismatch and Bursty Losses}
Finally, Fig.~\ref{fig:robustness} evaluates robustness under (i) mismatch in the assumed i.i.d.\ drop probability and (ii) bursty drops modeled by a Gilbert--Elliott channel.
We report normalized degradation relative to nominal performance.
While mismatch degrades all schedules, covariance-aware DP policies remain competitive and provide a useful oracle reference even when modeling assumptions are violated.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_D_robustness.pdf}
\caption{Robustness under channel mismatch (left) and bursty losses (right), shown as normalized degradation relative to nominal performance.
}
\label{fig:robustness}
\end{figure*}

\section{Conclusion}\label{sec:concl}
We formulated finite-horizon transmission scheduling with acknowledgements as a covariance-state MDP and derived the corresponding finite-horizon dynamic program.
The resulting optimal policy $\gamma_k^\star(P)$ admits a value-of-information characterization and serves as an oracle benchmark for finite-horizon performance--communication trade-offs under packet drops.
Because the covariance state is continuous, we described a practical offline computation procedure based on feature gridding and backward induction.
Future work will focus on scalable approximations for higher-dimensional systems, structured policy classes with performance guarantees, and learning-based approaches under unknown channel models.
