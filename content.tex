\begin{abstract}
Finite-horizon networked control tasks require explicit trade-offs between estimation quality and communication usage.
We study transmission scheduling for discrete-time linear--Gaussian systems operating over packet-dropping channels with acknowledgements.
Acknowledgements reveal whether a transmitted measurement is received, which makes the Kalman estimation error covariance a fully observed information state for communication decisions.
Using this structure, we cast finite-horizon scheduling as a Markov decision process (MDP) whose state is the covariance and whose action is a binary transmit decision.
We derive the finite-horizon Bellman recursion and an optimal time-varying policy $\gamma_k^\star(P)$ characterized by a value-of-information test.
Because the covariance state is continuous, we describe an offline dynamic-programming procedure based on gridding low-dimensional features of the covariance, yielding an oracle benchmark for finite-horizon performance--communication limits.
Numerical studies illustrate intrinsic finite-horizon effects and quantify gaps between covariance-aware scheduling and common scalar-surrogate heuristics.
\end{abstract}

\begin{IEEEkeywords}
networked control systems, remote estimation, transmission scheduling, Markov decision processes, packet drops, finite-horizon optimization
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
Wireless sensing and actuation links are fundamental components of modern cyber--physical systems.
In many applications, sensor measurements must be delivered to a controller over shared and unreliable wireless channels.
Periodic transmission can be wasteful under bandwidth or energy constraints, while unreliable links with packet drops make open-loop scheduling brittle.
This motivates \emph{transmission scheduling}: deciding \emph{when} to transmit measurements so as to balance control/estimation performance against communication usage.

Most existing scheduling formulations emphasize asymptotic stability or infinite-horizon average performance.
However, many practical scenarios are mission-driven and inherently finite-horizon: a robot must traverse a corridor in $30$\,s, an inspection drone has a short battery window, or a sensor network must complete a sweep under a limited packet budget.
In such settings, the \emph{remaining time} matters, and optimal scheduling decisions are generally time-varying.

This paper focuses on finite-horizon scheduling for linear systems over packet-dropping channels with acknowledgements (ACKs).
Under linear--Gaussian assumptions, ACKs allow the controller to track the Kalman estimation error covariance exactly.
We exploit this property to formulate scheduling as a finite-horizon MDP on the covariance information state and to compute an MDP-optimal (oracle) policy.
The objective is not to propose a low-complexity deployable scheduler, but to provide a principled finite-horizon benchmark and a clear computational recipe that can be used to design and evaluate practical heuristics.

\paragraph*{Contributions}
\begin{itemize}
\item We formulate finite-horizon transmission scheduling with ACKs as a fully observed covariance-state MDP.
\item We derive a finite-horizon dynamic program and an optimal policy $\gamma_k^\star(P)$ with an explicit value-of-information comparison.
\item We present an offline approximation procedure (feature gridding + backward induction) that produces an oracle benchmark and supports reproducible finite-horizon trade-off studies.
\end{itemize}

\section{Related Work}\label{sec:related}
Transmission scheduling and remote estimation over unreliable channels have been studied extensively in networked control and estimation.
Classic results analyze Kalman filtering under packet drops and identify conditions for bounded estimation error.
MDP-based sensor scheduling is also well developed under infinite-horizon discounted or average-cost criteria.
Event-triggered and self-triggered strategies provide low-complexity alternatives by transmitting only when a trigger condition is met, often yielding stability and communication savings.
In contrast, finite-horizon formulations explicitly encode the remaining time and can exhibit qualitatively different, time-varying optimal decisions.
This paper emphasizes finite-horizon benchmarking through a covariance-state MDP and an oracle dynamic program.


\section{System Model and Covariance Dynamics}
\label{sec:model}

We consider a discrete-time linear time-invariant (LTI) system
\begin{equation}
x_{k+1} = A x_k + B u_k + w_k,
\label{eq:lti}
\end{equation}
where $x_k\in\mathbb{R}^n$, $u_k\in\mathbb{R}^m$, and $\{w_k\}$ is zero-mean i.i.d.\ process noise with covariance $Q\succeq 0$.
The sensor measures
\begin{equation}
y_k = C x_k + v_k,
\label{eq:meas}
\end{equation}
where $\{v_k\}$ is zero-mean i.i.d.\ measurement noise with covariance $R\succ 0$.
We assume $(A,B)$ is stabilizable and $(A,C)$ is detectable.

\subsection{Intermittent Communication and Control Architecture}

At each time step $k$, a scheduler decides whether to attempt transmission of the current measurement.
This decision is represented by a binary variable
\begin{equation}
\gamma_k \in \{0,1\},
\end{equation}
where $\gamma_k=1$ requests transmission and $\gamma_k=0$ corresponds to silence.
Packet delivery is subject to random losses modeled by an i.i.d.\ Bernoulli process
\begin{equation}
\eta_k \in \{0,1\}, \qquad \Pr(\eta_k=1)=p,
\end{equation}
independent across time and independent of $(w_k,v_k)$.
The controller observes an acknowledgement (ACK), which reveals the effective reception indicator
\begin{equation}
\delta_k := \gamma_k \eta_k.
\end{equation}
Thus, a measurement update occurs if and only if $\delta_k=1$.

The controller applies a fixed certainty-equivalent state feedback law
\begin{equation}
u_k = -K \hat{x}_k,
\end{equation}
where $\hat{x}_k$ denotes the controller-side state estimate and the gain $K$ is chosen such that $A-BK$ is stable.
Throughout the paper, the control gain is held fixed and communication decisions affect closed-loop behavior only through the quality of state estimation.

\subsection{Controller-Side Estimation and Covariance Evolution}

The controller maintains a Kalman filter driven by intermittently received measurements.
Let
\begin{equation}
P_k := \mathbb{E}\!\left[(x_k-\hat{x}_k)(x_k-\hat{x}_k)^\top\right]
\end{equation}
denote the estimation error covariance available to the controller at decision time~$k$.
Given $P_k$, the one-step prediction operator is
\begin{equation}
\mathcal{T}(P_k) := A P_k A^\top + Q,
\end{equation}
and the corresponding measurement-update operator is
\begin{equation}
\mathcal{M}(P) := P - P C^\top (C P C^\top + R)^{-1} C P.
\end{equation}

Starting from $P_k$, the controller first forms the predicted covariance $\mathcal{T}(P_k)$.
If $\delta_k=1$, a Kalman update is applied; otherwise, the estimator remains in prediction mode.
The resulting covariance satisfies the recursion
\begin{equation}
P_{k+1} =
\begin{cases}
\mathcal{T}(P_k), & \delta_k=0,\\[1mm]
\mathcal{M}\!\big(\mathcal{T}(P_k)\big), & \delta_k=1.
\end{cases}
\label{eq:P_update}
\end{equation}
This grow-and-reset evolution captures how intermittent communication regulates the accumulation and contraction of estimation uncertainty.

\subsection{Covariance as an Information State}

Because acknowledgements reveal $\delta_k$, the controller can track the covariance $P_k$ exactly via~\eqref{eq:P_update}.
Under the linear--Gaussian model, the conditional distribution of future estimation errors depends on the past only through the current covariance.
As a result, $P_k$ constitutes a sufficient information state for scheduling decisions, and transmission scheduling can be formulated as a fully observed Markov decision process on the covariance state.

\section{Finite-Horizon Covariance-State MDP}
\label{sec:mdp}

We formulate transmission scheduling over a finite horizon as a Markov decision process with continuous, matrix-valued state.

\subsection{MDP Components}

The state at time $k$ is the estimation error covariance $s_k := P_k \in \mathbb{S}_+^n$, where $\mathbb{S}_+^n$ denotes the cone of symmetric positive semidefinite matrices.
The action is the binary scheduling decision $\gamma_k\in\{0,1\}$.

Given $(P_k,\gamma_k)$, the next covariance $P_{k+1}$ evolves according to~\eqref{eq:P_update}.
Equivalently, if $\gamma_k=0$, the transition is deterministic with $P_{k+1}=\mathcal{T}(P_k)$.
If $\gamma_k=1$, then
\[
P_{k+1} =
\begin{cases}
\mathcal{M}(\mathcal{T}(P_k)), & \text{with probability } p,\\
\mathcal{T}(P_k), & \text{with probability } 1-p.
\end{cases}
\]

We consider the stage cost
\begin{equation}
c(P_k,\gamma_k) := \mathrm{tr}(W P_k) + \lambda\,\gamma_k,
\end{equation}
where $W\succeq 0$ weights estimation uncertainty and $\lambda>0$ penalizes communication usage.
Unless otherwise stated, we use $W=I$, yielding $c(P,\gamma)=\mathrm{tr}(P)+\lambda\gamma$.

\subsection{Finite-Horizon Optimization Problem}

Fix a horizon $T$ and an initial covariance $P_0$.
A causal scheduling policy is a sequence $\pi=\{\pi_0,\ldots,\pi_{T-1}\}$ of measurable mappings
\begin{equation}
\gamma_k = \pi_k(P_k).
\end{equation}
The finite-horizon scheduling problem is
\begin{equation}
\min_{\pi}\ 
\mathbb{E}^{\pi}\!\left[
\sum_{k=0}^{T-1} \big(\mathrm{tr}(W P_k) + \lambda\,\gamma_k\big)
\right],
\label{eq:mdp_problem}
\end{equation}
subject to the covariance dynamics~\eqref{eq:P_update}.

The weighted cost~\eqref{eq:mdp_problem} can be interpreted as the Lagrangian relaxation of a hard communication budget constraint.
Varying $\lambda$ traces a finite-horizon performance--communication trade-off curve.

\section{Dynamic Programming and Oracle Policy Structure}
\label{sec:dp}

\subsection{Bellman Recursion}

Define the value functions
\begin{equation}
V_k(P)
:=
\min_{\pi_{k:T-1}}
\mathbb{E}\!\left[
\sum_{t=k}^{T-1} \big(\mathrm{tr}(W P_t) + \lambda\,\gamma_t\big)
\ \big|\ P_k=P
\right],
\end{equation}
with terminal condition $V_T(P)=0$.
Backward induction yields the Bellman recursion
\begin{align}
V_k(P)
&=
\min_{\gamma\in\{0,1\}}
\Big\{
\mathrm{tr}(W P) + \lambda\,\gamma
 + (1-p\gamma)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
 + p\gamma\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\Big\}.
\end{align}

\subsection{Value-of-Information Scheduling Rule}

For a given $V_{k+1}$, define
\begin{align}
Q_k(P,0) &:= \mathrm{tr}(W P) + V_{k+1}\!\big(\mathcal{T}(P)\big),\\
Q_k(P,1) &:= \mathrm{tr}(W P) + \lambda
 + (1-p)\,V_{k+1}\!\big(\mathcal{T}(P)\big)
 + p\,V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big).
\end{align}
The optimal action satisfies
\begin{equation}
\gamma_k^\star(P)=1
\quad \Longleftrightarrow \quad
\lambda < p\!\left(
V_{k+1}\!\big(\mathcal{T}(P)\big)
-
V_{k+1}\!\big(\mathcal{M}(\mathcal{T}(P))\big)
\right).
\label{eq:voi_test}
\end{equation}
The right-hand side represents the expected value of information from attempting a transmission: the probability of successful delivery times the reduction in cost-to-go achieved by a measurement update.
This oracle policy provides a finite-horizon benchmark for evaluating structured event-triggered heuristics.

\section{Offline Computation of the Oracle Benchmark}
\label{sec:compute}

The covariance state is continuous and matrix-valued, rendering exact dynamic programming intractable except for very small systems.
To obtain a computable oracle benchmark, we approximate the Bellman recursion on a finite set of representative covariance states.

\subsection{Grid-Based Backward Induction}

Let $\mathcal{G}=\{z^1,\ldots,z^N\}$ be a grid of representative covariances.
For each $z^i$, compute
\[
z^{i,\mathrm{pred}} := \mathcal{T}(z^i),
\qquad
z^{i,\mathrm{upd}} := \mathcal{M}(\mathcal{T}(z^i)),
\]
and map these successors to grid indices using a feature-based nearest-neighbor rule.
Backward induction then computes approximate value and policy tables by evaluating the Bellman update on the grid.

\subsection{Low-Dimensional Covariance Features}

To reduce complexity, the grid can be constructed over low-dimensional features of $P$.
In addition to a trace-only surrogate, we employ the two-dimensional feature map
\[
\phi(P)=
\begin{bmatrix}
\mathrm{tr}(P)\\
\log\det(P+\varepsilon I)
\end{bmatrix},
\]
which captures both total uncertainty and covariance shape.
Representative covariances are obtained by sampling reachable trajectories under random scheduling and selecting a prototype for each feature cell.
This approximation balances fidelity and tractability, enabling oracle computation over finite horizons.




\section{Numerical Studies}\label{sec:results}
We illustrate the finite-horizon benchmark and the resulting policy structure on representative remote-estimation experiments.
We report three metrics over a horizon $T$: (i) estimation cost $J_P:=\mathbb{E}[\sum_{k=0}^{T-1}\mathrm{tr}(P_k)]$, (ii) communication usage $J_C:=\mathbb{E}[\sum_{k=0}^{T-1}\gamma_k]$, and (iii) a closed-loop proxy $J_X:=\mathbb{E}[\sum_{k=0}^{T-1}\|x_k\|_2^2]$ under certainty-equivalent control~\eqref{eq:ce_control}.
The oracle policy is computed offline via the grid-based backward induction in Section~\ref{sec:compute}.

\subsection{Experimental Setup}
Unless otherwise stated, we use a planar double-integrator plant with sampling time $T_s=0.1$\,s and full-state sensing ($C=I$).
Process and measurement noise are Gaussian with standard deviations $(\sigma_w,\sigma_v)$, and packet delivery follows the i.i.d.\ model~\eqref{eq:drop} with success probability $p$.
We compare three schedulers:
(i) \textbf{DP-trace}, the trace-only finite-horizon DP from Section~\ref{sec:trace_surrogate};
(ii) \textbf{ET}, an event-triggered threshold rule $\gamma_k=\mathbf{1}\{\mathrm{tr}(P_k)>\delta\}$; and
(iii) \textbf{PER}, a periodic scheduler that transmits every $M$ steps.
All curves are obtained by Monte Carlo averaging over independent noise and packet-drop realizations.

\subsection{Finite-Horizon Trade-Off Curves}
Fig.~\ref{fig:tradeoff} reports finite-horizon trade-off curves obtained by sweeping $\lambda$ in~\eqref{eq:mdp_problem}.
The DP-trace benchmark achieves the best estimation and closed-loop costs among the compared low-complexity schedulers for a given communication level.
Importantly, the DP curve provides an actionable reference: it quantifies how far periodic and threshold-based policies lie from a finite-horizon dynamic-programming baseline at matched average communication.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_A_tradeoff_curves.pdf}
\caption{Finite-horizon performance--communication trade-offs obtained by sweeping $\lambda$.
Left: estimation cost $J_P$ vs.\ communication usage $J_C$.
Right: closed-loop proxy $J_X$ vs.\ $J_C$.
}
\label{fig:tradeoff}
\end{figure*}

\subsection{Trace Is Not Enough: A Counterexample}\label{sec:trace_not_enough}
The trace surrogate in Section~\ref{sec:trace_surrogate} can be strictly suboptimal because $\mathrm{tr}(P)$ discards directional information.
To illustrate this, we consider a $2$D anisotropic system with partial observation,
$A=\begin{bmatrix}1.08&0.10\\0&0.90\end{bmatrix}$,
$B=\begin{bmatrix}0.10\\0.05\end{bmatrix}$,
$C=\begin{bmatrix}1&0.25\end{bmatrix}$,
and compare \textbf{DP-trace} against \textbf{DP-2feature}, the two-feature discretization from Section~\ref{sec:feature_disc}.
\IfFileExists{figs/fig2_gap_curve.pdf}{
Fig.~\ref{fig:gapcurve} plots the estimation-cost gap under matched communication usage, showing that enriching the state with covariance-shape information can yield nontrivial improvements over trace-only dynamic programming.
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig2_gap_curve.pdf}
\caption{Trace is not enough: performance gap between trace-only DP and a two-feature DP (trace + logdet) under matched communication usage.}
\label{fig:gapcurve}
\end{figure*}
}{}

\subsection{Grow-and-Reset Dynamics Under Drops}
Fig.~\ref{fig:timeresp} shows a typical time response of $\mathrm{tr}(P_k)$ and the corresponding transmission attempts and receptions.
Between receptions, the covariance grows according to $\mathcal{T}(\cdot)$; a successful reception triggers an update $\mathcal{M}(\cdot)$ that ``resets'' uncertainty.
This grow-and-reset pattern is the mechanism exploited by scheduling policies, and it is captured exactly by the covariance-state MDP transition~\eqref{eq:P_update}.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/fig_B_time_response.pdf}
\caption{Time response illustrating covariance growth between receptions and resets upon successful updates.
Vertical markers indicate attempted transmissions and successful receptions (ACKs).
}
\label{fig:timeresp}
\end{figure}

\subsection{Sensitivity to Packet Success Probability}
Finite-horizon schedules designed for a nominal packet success probability $p$ can degrade when the channel quality changes.
Fig.~\ref{fig:sensitivity} compares \emph{no-retune} execution (deploying a nominal policy under a different $p$) against \emph{retune} execution (recomputing the policy using the true $p$).
The gap quantifies the value of adapting the scheduler to channel conditions, and highlights the role of $p$ in the value-of-information test~\eqref{eq:voi_test}.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_C_sensitivity_p.pdf}
\caption{Sensitivity to packet success probability $p$.
Retuning policies to the true $p$ improves both estimation cost $J_P$ and closed-loop proxy $J_X$ relative to deploying nominal schedules without retuning.
}
\label{fig:sensitivity}
\end{figure*}

\subsection{Robustness to Model Mismatch and Bursty Losses}
Finally, Fig.~\ref{fig:robustness} evaluates robustness under (i) mismatch in the assumed i.i.d.\ drop probability and (ii) bursty drops modeled by a Gilbert--Elliott channel.
We report normalized degradation relative to nominal performance.
While mismatch degrades all schedules, covariance-aware DP policies remain competitive and provide a useful oracle reference even when modeling assumptions are violated.

\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig_D_robustness.pdf}
\caption{Robustness under channel mismatch (left) and bursty losses (right), shown as normalized degradation relative to nominal performance.
}
\label{fig:robustness}
\end{figure*}

\section{Conclusion}\label{sec:concl}
We formulated finite-horizon transmission scheduling with acknowledgements as a covariance-state MDP and derived the corresponding finite-horizon dynamic program.
The resulting optimal policy $\gamma_k^\star(P)$ admits a value-of-information characterization and serves as an oracle benchmark for finite-horizon performance--communication trade-offs under packet drops.
Because the covariance state is continuous, we described a practical offline computation procedure based on feature gridding and backward induction.
Future work will focus on scalable approximations for higher-dimensional systems, structured policy classes with performance guarantees, and learning-based approaches under unknown channel models.
